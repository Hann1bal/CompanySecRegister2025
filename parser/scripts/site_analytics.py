import torch
import requests
import re
import json
from typing import List, Dict, Any, Set
import easyocr
from PIL import Image
from urllib.parse import urljoin, urlparse
from bs4 import BeautifulSoup
import time
from io import BytesIO


class EnhancedCompanySiteAnalyzer:
    def __init__(self, max_pages: int = 25, max_depth: int = 2):
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        self.max_pages = max_pages
        self.max_depth = max_depth
        self.visited_urls: Set[str] = set()
        self.session = requests.Session()

        # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω—ã–µ –ø—É—Ç–∏ –¥–ª—è –ø–æ–∏—Å–∫–∞ —Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–∞
        self.priority_paths = [
            '/about/', '/about', '/company/', '/company',
            '/team/', '/team', '/management/', '/management',
            '/ru/about/', '/ru/company/', '/ru/team/', '/ru/management/',
            '/o-nas/', '/o-kompanii/', '/komanda/', '/rukovodstvo/',
            '/directors/', '/executive/', '/leadership/',
            '/contacts/', '/contact/', '/kontakty/',
            '/partners/', '/clients/', '/customers/',
            '/dealer/', '/dealers/', '/dilers/', '/diler/'
        ]

        # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ —Å–µ—Å—Å–∏–∏
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
            'Accept-Language': 'ru-RU,ru;q=0.9,en;q=0.8',
        })
        self.russian_cities = [
            '–ú–æ—Å–∫–≤–∞', '–°–∞–Ω–∫—Ç-–ü–µ—Ç–µ—Ä–±—É—Ä–≥', '–ù–æ–≤–æ—Å–∏–±–∏—Ä—Å–∫', '–ï–∫–∞—Ç–µ—Ä–∏–Ω–±—É—Ä–≥', '–ù–∏–∂–Ω–∏–π –ù–æ–≤–≥–æ—Ä–æ–¥',
            '–ö–∞–∑–∞–Ω—å', '–ß–µ–ª—è–±–∏–Ω—Å–∫', '–û–º—Å–∫', '–°–∞–º–∞—Ä–∞', '–†–æ—Å—Ç–æ–≤-–Ω–∞-–î–æ–Ω—É', '–£—Ñ–∞', '–ö—Ä–∞—Å–Ω–æ—è—Ä—Å–∫',
            '–í–æ—Ä–æ–Ω–µ–∂', '–ü–µ—Ä–º—å', '–í–æ–ª–≥–æ–≥—Ä–∞–¥', '–ö—Ä–∞—Å–Ω–æ–¥–∞—Ä', '–°–∞—Ä–∞—Ç–æ–≤', '–¢—é–º–µ–Ω—å', '–¢–æ–ª—å—è—Ç—Ç–∏',
            '–ò–∂–µ–≤—Å–∫', '–ë–∞—Ä–Ω–∞—É–ª', '–£–ª—å—è–Ω–æ–≤—Å–∫', '–ò—Ä–∫—É—Ç—Å–∫', '–•–∞–±–∞—Ä–æ–≤—Å–∫', '–Ø—Ä–æ—Å–ª–∞–≤–ª—å', '–í–ª–∞–¥–∏–≤–æ—Å—Ç–æ–∫',
            '–ú–∞—Ö–∞—á–∫–∞–ª–∞', '–¢–æ–º—Å–∫', '–û—Ä–µ–Ω–±—É—Ä–≥', '–ö–µ–º–µ—Ä–æ–≤–æ', '–ù–æ–≤–æ–∫—É–∑–Ω–µ—Ü–∫', '–†—è–∑–∞–Ω—å', '–ê—Å—Ç—Ä–∞—Ö–∞–Ω—å',
            '–ù–∞–±–µ—Ä–µ–∂–Ω—ã–µ –ß–µ–ª–Ω—ã', '–ü–µ–Ω–∑–∞', '–õ–∏–ø–µ—Ü–∫', '–ö–∏—Ä–æ–≤', '–ß–µ–±–æ–∫—Å–∞—Ä—ã', '–¢—É–ª–∞', '–ö–∞–ª–∏–Ω–∏–Ω–≥—Ä–∞–¥',
            '–ë–∞–ª–∞—à–∏—Ö–∞', '–ö—É—Ä—Å–∫', '–°–µ–≤–∞—Å—Ç–æ–ø–æ–ª—å', '–°–æ—á–∏', '–°—Ç–∞–≤—Ä–æ–ø–æ–ª—å', '–£–ª–∞–Ω-–£–¥—ç', '–¢–≤–µ—Ä—å',
            '–ú–∞–≥–Ω–∏—Ç–æ–≥–æ—Ä—Å–∫', '–ò–≤–∞–Ω–æ–≤–æ', '–ë—Ä—è–Ω—Å–∫', '–ë–µ–ª–≥–æ—Ä–æ–¥', '–°—É—Ä–≥—É—Ç', '–í–ª–∞–¥–∏–º–∏—Ä', '–ù–∏–∂–Ω–∏–π –¢–∞–≥–∏–ª',
            '–ê—Ä—Ö–∞–Ω–≥–µ–ª—å—Å–∫', '–ß–∏—Ç–∞', '–ö–∞–ª—É–≥–∞', '–°–º–æ–ª–µ–Ω—Å–∫', '–í–æ–ª–∂—Å–∫–∏–π', '–Ø–∫—É—Ç—Å–∫', '–°–∞—Ä–∞–Ω—Å–∫',
            '–ü–æ–¥–æ–ª—å—Å–∫', '–ì—Ä–æ–∑–Ω—ã–π', '–û—Ä—ë–ª', '–ß–µ—Ä–µ–ø–æ–≤–µ—Ü', '–í–æ–ª–æ–≥–¥–∞', '–í–ª–∞–¥–∏–∫–∞–≤–∫–∞–∑', '–ú—É—Ä–º–∞–Ω—Å–∫',
            '–¢–∞–º–±–æ–≤', '–ü–µ—Ç—Ä–æ–∑–∞–≤–æ–¥—Å–∫', '–ù–∏–∂–Ω–µ–≤–∞—Ä—Ç–æ–≤—Å–∫', '–ö–æ—Å—Ç—Ä–æ–º–∞', '–ù–æ–≤–æ—Ä–æ—Å—Å–∏–π—Å–∫', '–ô–æ—à–∫–∞—Ä-–û–ª–∞',
            '–•–∏–º–∫–∏', '–¢–∞–≥–∞–Ω—Ä–æ–≥', '–°—ã–∫—Ç—ã–≤–∫–∞—Ä', '–ù–∞–ª—å—á–∏–∫', '–®–∞—Ö—Ç—ã', '–î–∑–µ—Ä–∂–∏–Ω—Å–∫', '–û—Ä—Å–∫', '–ë—Ä–∞—Ç—Å–∫',
            '–≠–Ω–≥–µ–ª—å—Å', '–ê–Ω–≥–∞—Ä—Å–∫', '–ö–æ—Ä–æ–ª—ë–≤', '–ü—Å–∫–æ–≤', '–ë–∏–π—Å–∫', '–ü—Ä–æ–∫–æ–ø—å–µ–≤—Å–∫', '–†—ã–±–∏–Ω—Å–∫',
            '–ë–∞–ª–∞–∫–æ–≤–æ', '–°–µ–≤–µ—Ä–æ–¥–≤–∏–Ω—Å–∫', '–ê—Ä–º–∞–≤–∏—Ä', '–ü–æ–¥–æ–ª—å—Å–∫', '–Æ–∂–Ω–æ-–°–∞—Ö–∞–ª–∏–Ω—Å–∫', '–ü–µ—Ç—Ä–æ–ø–∞–≤–ª–æ–≤—Å–∫-–ö–∞–º—á–∞—Ç—Å–∫–∏–π',
            '–°—ã–∑—Ä–∞–Ω—å', '–ù–æ—Ä–∏–ª—å—Å–∫', '–ó–ª–∞—Ç–æ—É—Å—Ç', '–ö–∞–º–µ–Ω—Å–∫-–£—Ä–∞–ª—å—Å–∫–∏–π', '–ú—ã—Ç–∏—â–∏', '–õ—é–±–µ—Ä—Ü—ã', '–í–æ–ª–≥–æ–¥–æ–Ω—Å–∫',
            '–ù–æ–≤–æ—á–µ—Ä–∫–∞—Å—Å–∫', '–ê–±–∞–∫–∞–Ω', '–ù–∞—Ö–æ–¥–∫–∞', '–£—Å—Å—É—Ä–∏–π—Å–∫', '–ë–µ—Ä–µ–∑–Ω–∏–∫–∏', '–°–∞–ª–∞–≤–∞—Ç', '–≠–ª–µ–∫—Ç—Ä–æ—Å—Ç–∞–ª—å',
            '–ú–∏–∞—Å—Å', '–ü–µ—Ä–≤–æ—É—Ä–∞–ª—å—Å–∫', '–ö–µ—Ä—á—å', '–ù–æ–≤–æ—É—Ä–∞–ª—å—Å–∫', '–ñ–µ–ª–µ–∑–Ω–æ–¥–æ—Ä–æ–∂–Ω—ã–π', '–ê–ª—å–º–µ—Ç—å–µ–≤—Å–∫',
            '–•–∞—Å–∞–≤—é—Ä—Ç', '–ö–æ–ø–µ–π—Å–∫', '–ü—è—Ç–∏–≥–æ—Ä—Å–∫', '–û–¥–∏–Ω—Ü–æ–≤–æ', '–†—É–±—Ü–æ–≤—Å–∫', '–ë–ª–∞–≥–æ–≤–µ—â–µ–Ω—Å–∫', '–ö–∏—Å–ª–æ–≤–æ–¥—Å–∫',
            '–ù–æ–≤–æ—à–∞—Ö—Ç–∏–Ω—Å–∫', '–ñ—É–∫–æ–≤—Å–∫–∏–π', '–°–µ–≤–µ—Ä—Å–∫', '–ù–∞–∑—Ä–∞–Ω—å', '–î–æ–º–æ–¥–µ–¥–æ–≤–æ', '–ö–∞—Å–ø–∏–π—Å–∫', '–ù–æ–≤–æ—Ç—Ä–æ–∏—Ü–∫'
        ]

        self.belarus_cities = [
            '–ú–∏–Ω—Å–∫', '–ì–æ–º–µ–ª—å', '–ú–æ–≥–∏–ª—ë–≤', '–í–∏—Ç–µ–±—Å–∫', '–ì—Ä–æ–¥–Ω–æ', '–ë—Ä–µ—Å—Ç', '–ë–æ–±—Ä—É–π—Å–∫', '–ë–∞—Ä–∞–Ω–æ–≤–∏—á–∏',
            '–ë–æ—Ä–∏—Å–æ–≤', '–ü–∏–Ω—Å–∫', '–û—Ä—à–∞', '–ú–æ–∑—ã—Ä—å', '–°–æ–ª–∏–≥–æ—Ä—Å–∫', '–ù–æ–≤–æ–ø–æ–ª–æ—Ü–∫', '–õ–∏–¥–∞', '–ú–æ–ª–æ–¥–µ—á–Ω–æ',
            '–ü–æ–ª–æ—Ü–∫', '–ñ–ª–æ–±–∏–Ω', '–°–≤–µ—Ç–ª–æ–≥–æ—Ä—Å–∫', '–†–µ—á–∏—Ü–∞', '–ñ–æ–¥–∏–Ω–æ', '–°–ª—É—Ü–∫', '–ö–æ–±—Ä–∏–Ω', '–í–æ–ª–∫–æ–≤—ã—Å–∫',
            '–ö–∞–ª–∏–Ω–∫–æ–≤–∏—á–∏', '–°–º–æ—Ä–≥–æ–Ω—å', '–û—Å–∏–ø–æ–≤–∏—á–∏', '–†–æ–≥–∞—á—ë–≤', '–ù–æ–≤–æ–≥—Ä—É–¥–æ–∫', '–ì–æ—Ä–∫–∏', '–ë–µ—Ä—ë–∑–∞',
            '–ò–≤–∞—Ü–µ–≤–∏—á–∏', '–õ—É–Ω–∏–Ω–µ—Ü', '–ü–æ—Å—Ç–∞–≤—ã', '–ß–∞—É—Å—ã', '–î–∑–µ—Ä–∂–∏–Ω—Å–∫', '–ú–∏–∫–∞—à–µ–≤–∏—á–∏', '–ë–µ–ª–æ–æ–∑—ë—Ä—Å–∫'
        ]
        print(f"üéØ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —É–ª—É—á—à–µ–Ω–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞")

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è OCR
        try:
            self.reader = easyocr.Reader(['ru', 'en'])
            print("‚úÖ OCR –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ OCR: {e}")
            self.reader = None

    def extract_slider_clients(self, html: str, base_url: str) -> Dict[str, List[str]]:
        """–ê–Ω–∞–ª–∏–∑ —Å–ª–∞–π–¥–µ—Ä–æ–≤ –∏ –∫–∞—Ä—É—Å–µ–ª–µ–π —Å –∫–ª–∏–µ–Ω—Ç–∞–º–∏ –∏ –¥–∏–ª–µ—Ä–∞–º–∏"""
        results = {
            "–∫–ª–∏–µ–Ω—Ç—ã": [],
            "–¥–∏–ª–µ—Ä—ã": [],
            "–ø–∞—Ä—Ç–Ω–µ—Ä—ã": []
        }

        if not self.reader:
            return results

        try:
            soup = BeautifulSoup(html, 'html.parser')

            # –°–µ–ª–µ–∫—Ç–æ—Ä—ã –¥–ª—è —Å–ª–∞–π–¥–µ—Ä–æ–≤ –∏ –∫–∞—Ä—É—Å–µ–ª–µ–π
            slider_selectors = [
                # –°–ª–∞–π–¥–µ—Ä—ã –∫–ª–∏–µ–Ω—Ç–æ–≤
                '.clients-slider', '.customers-slider', '.partners-slider',
                '.client-carousel', '.partner-carousel', '.brands-slider',
                '.logo-slider', '.clients-carousel',
                # –°–ª–∞–π–¥–µ—Ä—ã –¥–∏–ª–µ—Ä–æ–≤
                '.dealers-slider', '.dealer-carousel', '.dilers-slider',
                '.dealer-network', '.dealer-list',
                # –û–±—â–∏–µ —Å–ª–∞–π–¥–µ—Ä—ã
                '.slick-slider', '.owl-carousel', '.swiper-container',
                '[class*="slider"]', '[class*="carousel"]'
            ]

            for selector in slider_selectors:
                sliders = soup.select(selector)
                for slider in sliders[:5]:  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–ª–∞–π–¥–µ—Ä–æ–≤
                    print(f"üîç –ê–Ω–∞–ª–∏–∑ —Å–ª–∞–π–¥–µ—Ä–∞: {selector}")

                    # –ò–∑–≤–ª–µ–∫–∞–µ–º –≤—Å–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏–∑ —Å–ª–∞–π–¥–µ—Ä–∞
                    images = slider.find_all('img')
                    for img in images[:50]:  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
                        img_src = img.get('src')
                        if img_src:
                            img_url = urljoin(base_url, img_src)
                            try:
                                # –°–∫–∞—á–∏–≤–∞–µ–º –∏ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
                                response = self.session.get(img_url, timeout=10)
                                if response.status_code == 200:
                                    image = Image.open(BytesIO(response.content))

                                    # OCR –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞
                                    ocr_results = self.reader.readtext(image)
                                    for (bbox, text, confidence) in ocr_results:
                                        if confidence > 0.6 and len(text) > 2:
                                            cleaned_text = self._clean_company_name(text)
                                            if self._looks_like_company_name(cleaned_text):
                                                # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø —Å–ª–∞–π–¥–µ—Ä–∞ –ø–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç—É
                                                slider_type = self._classify_slider_type(slider, selector, text)
                                                if slider_type == "–∫–ª–∏–µ–Ω—Ç—ã":
                                                    results["–∫–ª–∏–µ–Ω—Ç—ã"].append(cleaned_text)
                                                elif slider_type == "–¥–∏–ª–µ—Ä—ã":
                                                    results["–¥–∏–ª–µ—Ä—ã"].append(cleaned_text)
                                                else:
                                                    results["–ø–∞—Ä—Ç–Ω–µ—Ä—ã"].append(cleaned_text)
                            except Exception as e:
                                continue

                    # –¢–∞–∫–∂–µ –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ç–µ–∫—Å—Ç–æ–≤–æ–µ —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ —Å–ª–∞–π–¥–µ—Ä–∞
                    slider_text = slider.get_text(strip=True)
                    if slider_text:
                        companies_from_text = self._extract_companies_from_slider_text(slider_text)
                        slider_type = self._classify_slider_type(slider, selector, slider_text)

                        if slider_type == "–∫–ª–∏–µ–Ω—Ç—ã":
                            results["–∫–ª–∏–µ–Ω—Ç—ã"].extend(companies_from_text)
                        elif slider_type == "–¥–∏–ª–µ—Ä—ã":
                            results["–¥–∏–ª–µ—Ä—ã"].extend(companies_from_text)
                        else:
                            results["–ø–∞—Ä—Ç–Ω–µ—Ä—ã"].extend(companies_from_text)

            # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã
            for key in results:
                results[key] = list(set(results[key]))

            print(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ –≤ —Å–ª–∞–π–¥–µ—Ä–∞—Ö: {sum(len(v) for v in results.values())} –∫–æ–º–ø–∞–Ω–∏–π")

        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ —Å–ª–∞–π–¥–µ—Ä–æ–≤: {e}")

        return results

    def extract_representatives_from_contacts_pages(self, text: str, html: str = "", url: str = "") -> List[
        Dict[str, Any]]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏—Ç–µ–ª—å—Å—Ç–≤ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–∞—Ö –∫–æ–Ω—Ç–∞–∫—Ç–æ–≤/–¥–∏–ª–µ—Ä–æ–≤ –ø–æ –ø–∞—Ç—Ç–µ—Ä–Ω–∞–º –∫–æ–º–ø–∞–Ω–∏–π"""
        representatives = []

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —ç—Ç–æ —Å—Ç—Ä–∞–Ω–∏—Ü–µ–π –∫–æ–Ω—Ç–∞–∫—Ç–æ–≤/–¥–∏–ª–µ—Ä–æ–≤
        if not self._is_contacts_or_dealers_page(url, text):
            return representatives

        print("üîç –ê–Ω–∞–ª–∏–∑ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∫–æ–Ω—Ç–∞–∫—Ç–æ–≤/–¥–∏–ª–µ—Ä–æ–≤ –Ω–∞ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏—Ç–µ–ª—å—Å—Ç–≤–∞...")

        # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –ø–æ–∏—Å–∫–∞ –∫–æ–º–ø–∞–Ω–∏–π-–ø—Ä–µ–¥—Å—Ç–∞–≤–∏—Ç–µ–ª–µ–π
        company_patterns = [
            # –û–û–û "–ù–∞–∑–≤–∞–Ω–∏–µ", –û–ê–û "–ù–∞–∑–≤–∞–Ω–∏–µ" –∏ —Ç.–¥.
            r'(–û–û–û|–û–ê–û|–ò–ü|–ó–ê–û|–ê–û|–ü–ê–û)\s+[¬´"]([^¬ª"]+?)[¬ª"]',
            r'[¬´"]([^¬ª"]+?)[¬ª"]\s+(–û–û–û|–û–ê–û|–ò–ü|–ó–ê–û|–ê–û|–ü–ê–û)',
            # –ë–µ–∑ –∫–∞–≤—ã—á–µ–∫
            r'(–û–û–û|–û–ê–û|–ò–ü|–ó–ê–û|–ê–û|–ü–ê–û)\s+([–ê-–Ø][A-Z–ê-–Øa-z–∞-—è\s&]{3,50})',
            r'([–ê-–Ø][A-Z–ê-–Øa-z–∞-—è\s&]{3,50})\s+(–û–û–û|–û–ê–û|–ò–ü|–ó–ê–û|–ê–û|–ü–ê–û)',
        ]

        # –ü–æ–∏—Å–∫ –∫–æ–º–ø–∞–Ω–∏–π –≤ —Ç–µ–∫—Å—Ç–µ
        found_companies = []
        for pattern in company_patterns:
            matches = re.findall(pattern, text, re.IGNORECASE)
            for match in matches:
                if len(match) == 2:
                    company_type, company_name = match
                    if self._looks_like_company_name(company_name):
                        found_companies.append({
                            "—Ç–∏–ø_–∫–æ–º–ø–∞–Ω–∏–∏": company_type.upper(),
                            "–Ω–∞–∑–≤–∞–Ω–∏–µ": company_name.strip(),
                            "–∫–æ–Ω—Ç–µ–∫—Å—Ç": self._extract_company_context(text, company_name)
                        })

        # –ü–æ–∏—Å–∫ —Å–∞–π—Ç–æ–≤ –∏ –∞–¥—Ä–µ—Å–æ–≤ —Ä—è–¥–æ–º —Å –Ω–∞–π–¥–µ–Ω–Ω—ã–º–∏ –∫–æ–º–ø–∞–Ω–∏—è–º–∏
        for company in found_companies:
            company_info = self._extract_company_details(text, html, company["–Ω–∞–∑–≤–∞–Ω–∏–µ"])
            if company_info["—Å–∞–π—Ç"] or company_info["–∞–¥—Ä–µ—Å"]:
                representatives.append({
                    "–Ω–∞–∑–≤–∞–Ω–∏–µ_–∫–æ–º–ø–∞–Ω–∏–∏": f"{company['—Ç–∏–ø_–∫–æ–º–ø–∞–Ω–∏–∏']} {company['–Ω–∞–∑–≤–∞–Ω–∏–µ']}",
                    "—Å–∞–π—Ç": company_info["—Å–∞–π—Ç"],
                    "–∞–¥—Ä–µ—Å": company_info["–∞–¥—Ä–µ—Å"],
                    "—Ç–µ–ª–µ—Ñ–æ–Ω": company_info["—Ç–µ–ª–µ—Ñ–æ–Ω"],
                    "—Ç–∏–ø": "–∫–æ–º–ø–∞–Ω–∏—è-–ø—Ä–µ–¥—Å—Ç–∞–≤–∏—Ç–µ–ª—å",
                    "–∏—Å—Ç–æ—á–Ω–∏–∫": "—Å—Ç—Ä–∞–Ω–∏—Ü–∞_–∫–æ–Ω—Ç–∞–∫—Ç–æ–≤",
                    "–∫–æ–Ω—Ç–µ–∫—Å—Ç": company["–∫–æ–Ω—Ç–µ–∫—Å—Ç"]
                })

        return representatives

    def _is_contacts_or_dealers_page(self, url: str, text: str) -> bool:
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Å—Ç—Ä–∞–Ω–∏—Ü–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ–π –∫–æ–Ω—Ç–∞–∫—Ç–æ–≤ –∏–ª–∏ –¥–∏–ª–µ—Ä–æ–≤"""
        url_lower = url.lower()
        text_lower = text.lower()

        # –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –≤ URL
        url_keywords = [
            'contacts', 'contact', 'kontakty', 'kontakt',
            'dealers', 'dealer', 'dilers', 'diler',
            'partners', 'partner', 'offices', 'office',
            'where-to-buy', 'distributors', 'distributor'
        ]

        # –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –≤ —Ç–µ–∫—Å—Ç–µ
        text_keywords = [
            '–∫–æ–Ω—Ç–∞–∫—Ç—ã', '–¥–∏–ª–µ—Ä—ã', '–¥–∏–ª–ª–µ—Ä—ã', '–ø–∞—Ä—Ç–Ω–µ—Ä—ã',
            '–≥–¥–µ –∫—É–ø–∏—Ç—å', '–∞–¥—Ä–µ—Å–∞', '–æ—Ñ–∏—Å—ã', '–ø—Ä–µ–¥—Å—Ç–∞–≤–∏—Ç–µ–ª—å—Å—Ç–≤–∞',
            '–¥–∏–ª–µ—Ä—Å–∫–∞—è —Å–µ—Ç—å', '—Å–µ—Ç—å –ø—Ä–æ–¥–∞–∂', '—Ä–µ–≥–∏–æ–Ω–∞–ª—å–Ω—ã–µ –ø–∞—Ä—Ç–Ω–µ—Ä—ã'
        ]

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º URL
        if any(keyword in url_lower for keyword in url_keywords):
            return True

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∑–∞–≥–æ–ª–æ–≤–∫–∏ –∏ —Ç–µ–∫—Å—Ç
        if any(keyword in text_lower for keyword in text_keywords):
            return True

        return False

    def _extract_company_context(self, text: str, company_name: str, context_size: int = 200) -> str:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç –≤–æ–∫—Ä—É–≥ –Ω–∞–∑–≤–∞–Ω–∏—è –∫–æ–º–ø–∞–Ω–∏–∏"""
        start_idx = text.find(company_name)
        if start_idx == -1:
            return ""

        end_idx = start_idx + len(company_name)
        context_start = max(0, start_idx - context_size)
        context_end = min(len(text), end_idx + context_size)

        context = text[context_start:context_end].strip()
        # –û—á–∏—â–∞–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –æ—Ç –ª–∏—à–Ω–∏—Ö –ø—Ä–æ–±–µ–ª–æ–≤
        context = re.sub(r'\s+', ' ', context)
        return context

    def _extract_company_details(self, text: str, html: str, company_name: str) -> Dict[str, str]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∫–æ–Ω—Ç–∞–∫—Ç–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –∫–æ–º–ø–∞–Ω–∏–∏ (—Å–∞–π—Ç, –∞–¥—Ä–µ—Å, —Ç–µ–ª–µ—Ñ–æ–Ω)"""
        details = {
            "—Å–∞–π—Ç": "",
            "–∞–¥—Ä–µ—Å": "",
            "—Ç–µ–ª–µ—Ñ–æ–Ω": ""
        }

        # –ü–æ–∏—Å–∫ –≤ —Ç–µ–∫—Å—Ç–µ
        company_context = self._extract_company_context(text, company_name, 300)

        # –ü–æ–∏—Å–∫ —Å–∞–π—Ç–∞
        website_patterns = [
            r'(?:www\.|https?://)[^\s,]+',
            r'(?:—Å–∞–π—Ç|website)[:\s]*([^\s,]+)',
            r'[a-zA-Z0-9-]+\.[a-zA-Z]{2,}(?:\.[a-zA-Z]{2,})?'
        ]

        for pattern in website_patterns:
            websites = re.findall(pattern, company_context, re.IGNORECASE)
            for website in websites:
                if self._is_valid_website(website):
                    details["—Å–∞–π—Ç"] = website
                    break
            if details["—Å–∞–π—Ç"]:
                break

        # –ü–æ–∏—Å–∫ –∞–¥—Ä–µ—Å–∞
        address_patterns = [
            r'(?:–∞–¥—Ä–µ—Å|address)[:\s]*([–ê-–Ø–∞-—è0-9\s,.-]{10,100})',
            r'(?:—é—Ä–∏–¥–∏—á–µ—Å–∫–∏–π –∞–¥—Ä–µ—Å|legal address)[:\s]*([–ê-–Ø–∞-—è0-9\s,.-]{10,100})',
            r'[–ê-–Ø–∞-—è0-9\s,.-]{10,100}?(?:—É–ª|—É–ª–∏—Ü–∞|–ø—Ä|–ø—Ä–æ—Å–ø–µ–∫—Ç|–ø–µ—Ä|–ø–µ—Ä–µ—É–ª–æ–∫)[^,.]{5,50}'
        ]

        for pattern in address_patterns:
            addresses = re.findall(pattern, company_context, re.IGNORECASE)
            if addresses:
                details["–∞–¥—Ä–µ—Å"] = addresses[0].strip()
                break

        # –ü–æ–∏—Å–∫ —Ç–µ–ª–µ—Ñ–æ–Ω–∞
        phone_patterns = [
            r'[\+]?[7|8][\s(-]?\d{3}[\s)-]?\d{3}[\s-]?\d{2}[\s-]?\d{2}',
            r'\(\d{3,4}\)\s?\d{2,3}[\s-]?\d{2}[\s-]?\d{2}',
        ]

        for pattern in phone_patterns:
            phones = re.findall(pattern, company_context)
            if phones:
                details["—Ç–µ–ª–µ—Ñ–æ–Ω"] = phones[0]
                break

        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –ø–æ–∏—Å–∫ –≤ HTML
        if html:
            html_details = self._extract_company_details_from_html(html, company_name)
            if not details["—Å–∞–π—Ç"] and html_details["—Å–∞–π—Ç"]:
                details["—Å–∞–π—Ç"] = html_details["—Å–∞–π—Ç"]
            if not details["–∞–¥—Ä–µ—Å"] and html_details["–∞–¥—Ä–µ—Å"]:
                details["–∞–¥—Ä–µ—Å"] = html_details["–∞–¥—Ä–µ—Å"]
            if not details["—Ç–µ–ª–µ—Ñ–æ–Ω"] and html_details["—Ç–µ–ª–µ—Ñ–æ–Ω"]:
                details["—Ç–µ–ª–µ—Ñ–æ–Ω"] = html_details["—Ç–µ–ª–µ—Ñ–æ–Ω"]

        return details

    def _extract_company_details_from_html(self, html: str, company_name: str) -> Dict[str, str]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∫–æ–Ω—Ç–∞–∫—Ç–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –∫–æ–º–ø–∞–Ω–∏–∏ –∏–∑ HTML —Å—Ç—Ä—É–∫—Ç—É—Ä—ã"""
        details = {"—Å–∞–π—Ç": "", "–∞–¥—Ä–µ—Å": "", "—Ç–µ–ª–µ—Ñ–æ–Ω": ""}

        try:
            soup = BeautifulSoup(html, 'html.parser')

            # –ò—â–µ–º —ç–ª–µ–º–µ–Ω—Ç—ã, —Å–æ–¥–µ—Ä–∂–∞—â–∏–µ –Ω–∞–∑–≤–∞–Ω–∏–µ –∫–æ–º–ø–∞–Ω–∏–∏
            company_elements = soup.find_all(string=re.compile(re.escape(company_name), re.IGNORECASE))

            for element in company_elements:
                parent = element.parent
                if parent:
                    # –ò—â–µ–º –∫–æ–Ω—Ç–∞–∫—Ç–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –≤ —Ä–æ–¥–∏—Ç–µ–ª—å—Å–∫–æ–º —ç–ª–µ–º–µ–Ω—Ç–µ –∏ —Å–æ—Å–µ–¥–Ω–∏—Ö
                    context_text = parent.get_text()

                    # –ü–æ–∏—Å–∫ —Å–∞–π—Ç–∞
                    websites = re.findall(r'(?:www\.|https?://)[^\s,]+', context_text)
                    for website in websites:
                        if self._is_valid_website(website) and not details["—Å–∞–π—Ç"]:
                            details["—Å–∞–π—Ç"] = website

                    # –ü–æ–∏—Å–∫ –∞–¥—Ä–µ—Å–∞
                    addresses = re.findall(r'(?:–∞–¥—Ä–µ—Å|address)[:\s]*([–ê-–Ø–∞-—è0-9\s,.-]{10,100})', context_text,
                                           re.IGNORECASE)
                    if addresses and not details["–∞–¥—Ä–µ—Å"]:
                        details["–∞–¥—Ä–µ—Å"] = addresses[0].strip()

                    # –ü–æ–∏—Å–∫ —Ç–µ–ª–µ—Ñ–æ–Ω–∞
                    phones = re.findall(r'[\+]?[7|8][\s(-]?\d{3}[\s)-]?\d{3}[\s-]?\d{2}[\s-]?\d{2}', context_text)
                    if phones and not details["—Ç–µ–ª–µ—Ñ–æ–Ω"]:
                        details["—Ç–µ–ª–µ—Ñ–æ–Ω"] = phones[0]

        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –∫–æ–º–ø–∞–Ω–∏–∏ –∏–∑ HTML: {e}")

        return details

    def _is_valid_website(self, website: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –≤–∞–ª–∏–¥–Ω–æ—Å—Ç—å –≤–µ–±-—Å–∞–π—Ç–∞"""
        website = website.strip()

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ–º–µ–Ω
        domain_patterns = [
            r'^[a-zA-Z0-9-]+\.[a-zA-Z]{2,}$',
            r'^www\.[a-zA-Z0-9-]+\.[a-zA-Z]{2,}$',
            r'^https?://[a-zA-Z0-9-]+\.[a-zA-Z]{2,}',
            r'^[a-zA-Z0-9-]+\.[a-zA-Z]{2,}\.[a-zA-Z]{2,}$'
        ]

        for pattern in domain_patterns:
            if re.match(pattern, website):
                return True

        return False

    def extract_representatives(self, text: str, html: str = "") -> List[Dict[str, Any]]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏—Ç–µ–ª—å—Å—Ç–≤–∞—Ö –∏ —Ñ–∏–ª–∏–∞–ª–∞—Ö —Å —É–ª—É—á—à–µ–Ω–Ω—ã–º –ø–æ–∏—Å–∫–æ–º –≥–æ—Ä–æ–¥–æ–≤"""
        representatives = []

        # –°–ø–∏—Å–∫–∏ –≥–æ—Ä–æ–¥–æ–≤ –†–æ—Å—Å–∏–∏ –∏ –ë–µ–ª–æ—Ä—É—Å—Å–∏–∏ –¥–ª—è —Ç–æ—á–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞


        all_cities = self.russian_cities + self.belarus_cities
        cities_pattern = '|'.join(all_cities)

        # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –ø–æ–∏—Å–∫–∞ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏—Ç–µ–ª—å—Å—Ç–≤ —Å —É—á–µ—Ç–æ–º —Å—É—Ñ—Ñ–∏–∫—Å–æ–≤ –∏ –æ–∫–æ–Ω—á–∞–Ω–∏–π
        patterns = [
            # –ü—Ä–µ–¥—Å—Ç–∞–≤–∏—Ç–µ–ª—å—Å—Ç–≤–æ –≤ –≥–æ—Ä–æ–¥–µ (—Ç–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ)
            r'(?:–ø—Ä–µ–¥—Å—Ç–∞–≤–∏—Ç–µ–ª—å—Å—Ç–≤–æ|—Ñ–∏–ª–∏–∞–ª|–æ—Ñ–∏—Å|–æ—Ç–¥–µ–ª–µ–Ω–∏–µ|–¥–∏–ª–µ—Ä—Å–∫–∏–π —Ü–µ–Ω—Ç—Ä|—Å–µ—Ä–≤–∏—Å–Ω—ã–π —Ü–µ–Ω—Ç—Ä)\s+(?:–≤|–≥\.?|–≥–æ—Ä–æ–¥–µ?)\s*[¬´"]?(' + cities_pattern + ')[¬ª"]?',

            # –ì–æ—Ä–æ–¥ —Å —Å—É—Ñ—Ñ–∏–∫—Å–∞–º–∏ –∏ –æ–∫–æ–Ω—á–∞–Ω–∏—è–º–∏
            r'(?:–≥\.|–≥–æ—Ä–æ–¥)\s*(' + '|'.join([re.escape(city) for city in
                                             all_cities]) + ')(?:—Å–∫–∏–π|—Å–∫–æ–π|–Ω—ã–π|–Ω–æ–µ|—Å–∫–∞—è|—Å–∫–æ–µ|–æ–≤–æ|–µ–≤–æ|–∏–Ω–æ|–Ω–æ|—Å–∫|—Ü–∫|—å–∫)?\b',

            # –ê–¥—Ä–µ—Å–∞ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏—Ç–µ–ª—å—Å—Ç–≤ —Å –≥–æ—Ä–æ–¥–∞–º–∏
            r'(?:–ø—Ä–µ–¥—Å—Ç–∞–≤–∏—Ç–µ–ª—å—Å—Ç–≤–æ|—Ñ–∏–ª–∏–∞–ª|–æ—Ñ–∏—Å)[^.!?]{0,200}?(?:–≥\.|–≥–æ—Ä–æ–¥)\s*(' + cities_pattern + ')[^.!?]{0,200}?([–ê-–Ø–∞-—è0-9\s,.-]{10,100}?)(?=[.!]|$)',

            # –ù–∞–∑–≤–∞–Ω–∏—è –∫–æ–º–ø–∞–Ω–∏–π-–ø—Ä–µ–¥—Å—Ç–∞–≤–∏—Ç–µ–ª–µ–π —Å —É–∫–∞–∑–∞–Ω–∏–µ–º –≥–æ—Ä–æ–¥–æ–≤
            r'(?:–æ–æ–æ|–∑–∞–æ|–∞–æ|–ø–∞–æ)\s+[¬´"]?([^¬ª"]+?)[¬ª"]?(?:\s+[^.!?]{0,150}?(?:–≤|–≥\.|–≥–æ—Ä–æ–¥)\s*(' + cities_pattern + '))',

            # –ö–æ–Ω—Ç–∞–∫—Ç—ã –ø—Ä–µ–¥—Å—Ç–∞–≤–∏—Ç–µ–ª—å—Å—Ç–≤
            r'(?:–∫–æ–Ω—Ç–∞–∫—Ç—ã|–∞–¥—Ä–µ—Å)[^.!?]{0,150}?(?:–≤|–≥\.|–≥–æ—Ä–æ–¥)\s*(' + cities_pattern + ')[^.!?]{0,150}?([–ê-–Ø–∞-—è0-9\s,.-]{10,80})',

            # –†–µ–≥–∏–æ–Ω–∞–ª—å–Ω—ã–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏—Ç–µ–ª—å—Å—Ç–≤–∞
            r'(?:—Ä–µ–≥–∏–æ–Ω–∞–ª—å–Ω|–æ–±–ª–∞—Å—Ç–Ω|–≥–æ—Ä–æ–¥—Å–∫)(?:–æ–µ|–æ–π|–∞—è)\s+(?:–ø—Ä–µ–¥—Å—Ç–∞–≤–∏—Ç–µ–ª—å—Å—Ç–≤–æ|—Ñ–∏–ª–∏–∞–ª|–æ—Ñ–∏—Å)[^.!?]{0,100}?(?:–≤|–≥\.|–≥–æ—Ä–æ–¥)\s*(' + cities_pattern + ')',
        ]

        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –≥–æ—Ä–æ–¥–æ–≤ —Å —Ä–∞–∑–Ω—ã–º–∏ –æ–∫–æ–Ω—á–∞–Ω–∏—è–º–∏
        city_variations = []
        for city in all_cities:
            # –£–±–∏—Ä–∞–µ–º –æ–∫–æ–Ω—á–∞–Ω–∏—è –¥–ª—è –±–∞–∑–æ–≤–æ–π —Ñ–æ—Ä–º—ã
            base_city = re.sub(r'(?:—Å–∫–∏–π|—Å–∫–æ–π|–Ω—ã–π|–Ω–æ–µ|—Å–∫–∞—è|—Å–∫–æ–µ|–æ–≤–æ|–µ–≤–æ|–∏–Ω–æ|–Ω–æ|—Å–∫|—Ü–∫|—å–∫)$', '', city)
            if base_city != city:
                city_variations.append(base_city)

        if city_variations:
            variations_pattern = '|'.join(city_variations)
            patterns.append(
                r'(?:–ø—Ä–µ–¥—Å—Ç–∞–≤–∏—Ç–µ–ª—å—Å—Ç–≤–æ|—Ñ–∏–ª–∏–∞–ª|–æ—Ñ–∏—Å)\s+(?:–≤|–≥\.?|–≥–æ—Ä–æ–¥–µ?)\s*[¬´"]?(' + variations_pattern + ')[¬ª"]?'
            )

        for pattern in patterns:
            matches = re.findall(pattern, text, re.IGNORECASE | re.DOTALL)
            for match in matches:
                if isinstance(match, tuple):
                    if len(match) == 2:
                        city, address = match
                        # –ù–∞—Ö–æ–¥–∏–º —Ç–æ—á–Ω–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ –≥–æ—Ä–æ–¥–∞
                        exact_city = self._find_exact_city(city, all_cities)
                        if exact_city:
                            representatives.append({
                                "–≥–æ—Ä–æ–¥": exact_city,
                                "–∞–¥—Ä–µ—Å": self._clean_address(address),
                                "—Ç–∏–ø": "–ø—Ä–µ–¥—Å—Ç–∞–≤–∏—Ç–µ–ª—å—Å—Ç–≤–æ",
                                "–∏—Å—Ç–æ—á–Ω–∏–∫": "—Ç–µ–∫—Å—Ç",
                                "—Å—Ç—Ä–∞–Ω–∞": "–†–æ—Å—Å–∏—è" if exact_city in self.russian_cities else "–ë–µ–ª–∞—Ä—É—Å—å"
                            })
                    elif len(match) >= 1:
                        city_or_company = match[0]
                        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —ç—Ç–æ –≥–æ—Ä–æ–¥–æ–º
                        exact_city = self._find_exact_city(city_or_company, all_cities)
                        if exact_city:
                            representatives.append({
                                "–≥–æ—Ä–æ–¥": exact_city,
                                "–∞–¥—Ä–µ—Å": "",
                                "—Ç–∏–ø": "–ø—Ä–µ–¥—Å—Ç–∞–≤–∏—Ç–µ–ª—å—Å—Ç–≤–æ",
                                "–∏—Å—Ç–æ—á–Ω–∏–∫": "—Ç–µ–∫—Å—Ç",
                                "—Å—Ç—Ä–∞–Ω–∞": "–†–æ—Å—Å–∏—è" if exact_city in self.russian_cities else "–ë–µ–ª–∞—Ä—É—Å—å"
                            })
                        else:
                            # –≠—Ç–æ –Ω–∞–∑–≤–∞–Ω–∏–µ –∫–æ–º–ø–∞–Ω–∏–∏
                            representatives.append({
                                "–Ω–∞–∑–≤–∞–Ω–∏–µ_–∫–æ–º–ø–∞–Ω–∏–∏": city_or_company.strip(),
                                "—Ç–∏–ø": "–∫–æ–º–ø–∞–Ω–∏—è-–ø—Ä–µ–¥—Å—Ç–∞–≤–∏—Ç–µ–ª—å",
                                "–∏—Å—Ç–æ—á–Ω–∏–∫": "—Ç–µ–∫—Å—Ç"
                            })
                else:
                    exact_city = self._find_exact_city(match, all_cities)
                    if exact_city:
                        representatives.append({
                            "–≥–æ—Ä–æ–¥": exact_city,
                            "–∞–¥—Ä–µ—Å": "",
                            "—Ç–∏–ø": "–ø—Ä–µ–¥—Å—Ç–∞–≤–∏—Ç–µ–ª—å—Å—Ç–≤–æ",
                            "–∏—Å—Ç–æ—á–Ω–∏–∫": "—Ç–µ–∫—Å—Ç",
                            "—Å—Ç—Ä–∞–Ω–∞": "–†–æ—Å—Å–∏—è" if exact_city in self.russian_cities else "–ë–µ–ª–∞—Ä—É—Å—å"
                        })

        # –ü–æ–∏—Å–∫ –≤ HTML —Å—Ç—Ä—É–∫—Ç—É—Ä–µ
        if html:
            html_representatives = self._extract_representatives_from_html(html, all_cities)
            representatives.extend(html_representatives)

        # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã
        unique_repr = []
        seen = set()
        for repr in representatives:
            key = f"{repr.get('–≥–æ—Ä–æ–¥', '')}_{repr.get('–Ω–∞–∑–≤–∞–Ω–∏–µ_–∫–æ–º–ø–∞–Ω–∏–∏', '')}_{repr.get('–∞–¥—Ä–µ—Å', '')}"
            if key not in seen and key != "__":
                unique_repr.append(repr)
                seen.add(key)

        print(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(unique_repr)} –ø—Ä–µ–¥—Å—Ç–∞–≤–∏—Ç–µ–ª—å—Å—Ç–≤")
        return unique_repr

    def _find_exact_city(self, city_variant: str, all_cities: List[str]) -> str:
        """–ù–∞—Ö–æ–¥–∏—Ç —Ç–æ—á–Ω–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ –≥–æ—Ä–æ–¥–∞ –ø–æ –≤–∞—Ä–∏–∞–Ω—Ç—É —Å –æ–∫–æ–Ω—á–∞–Ω–∏—è–º–∏"""
        city_variant_clean = re.sub(r'[^\w\s]', '', city_variant).strip()

        # –ü—Ä—è–º–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ
        for city in all_cities:
            if city.lower() == city_variant_clean.lower():
                return city

        # –°–æ–≤–ø–∞–¥–µ–Ω–∏–µ —Å —É—á–µ—Ç–æ–º –æ–∫–æ–Ω—á–∞–Ω–∏–π
        for city in all_cities:
            base_city = re.sub(r'(?:—Å–∫–∏–π|—Å–∫–æ–π|–Ω—ã–π|–Ω–æ–µ|—Å–∫–∞—è|—Å–∫–æ–µ|–æ–≤–æ|–µ–≤–æ|–∏–Ω–æ|–Ω–æ|—Å–∫|—Ü–∫|—å–∫)$', '', city)
            if base_city.lower() == city_variant_clean.lower():
                return city

        # –ß–∞—Å—Ç–∏—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ (–¥–ª—è —Å–ª—É—á–∞–µ–≤ —Ç–∏–ø–∞ "–ú–æ—Å–∫–≤" –≤–º–µ—Å—Ç–æ "–ú–æ—Å–∫–≤–∞")
        for city in all_cities:
            if city.lower().startswith(city_variant_clean.lower()) and len(city_variant_clean) >= 4:
                return city
            if city_variant_clean.lower().startswith(city.lower()) and len(city) >= 4:
                return city

        return ""

    def _clean_address(self, address: str) -> str:
        """–û—á–∏—â–∞–µ—Ç –∏ —Ñ–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç –∞–¥—Ä–µ—Å"""
        if not address:
            return ""

        # –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ —Å–ª–æ–≤–∞
        clean_address = re.sub(r'(?:—Ç–µ–ª|—Ç–µ–ª–µ—Ñ–æ–Ω|phone|email|@|www\.)[^,.]*', '', address, flags=re.IGNORECASE)
        # –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã
        clean_address = re.sub(r'\s+', ' ', clean_address)
        # –û–±—Ä–µ–∑–∞–µ–º –¥–æ —Ä–∞–∑—É–º–Ω–æ–π –¥–ª–∏–Ω—ã
        clean_address = clean_address.strip()

        if len(clean_address) > 150:
            clean_address = clean_address[:147] + "..."

        return clean_address

    def _extract_representatives_from_html(self, html: str, all_cities: List[str]) -> List[Dict[str, Any]]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏—Ç–µ–ª—å—Å—Ç–≤ –∏–∑ HTML —Å—Ç—Ä—É–∫—Ç—É—Ä—ã —Å —É—á–µ—Ç–æ–º –≥–æ—Ä–æ–¥–æ–≤"""
        representatives = []

        try:
            soup = BeautifulSoup(html, 'html.parser')

            # –°–µ–ª–µ–∫—Ç–æ—Ä—ã –¥–ª—è —Ä–∞–∑–¥–µ–ª–æ–≤ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏—Ç–µ–ª—å—Å—Ç–≤
            representative_selectors = [
                '.representatives', '.offices', '.contacts-list', '.branches',
                '.filials', '.regional-offices', '.dealer-centers', '.service-centers',
                '[class*="representative"]', '[class*="branch"]', '[class*="filial"]',
                '[class*="office"]', '[class*="dealer"]', '[class*="service"]',
                '.city-list', '.regional-contacts', '.offices-list'
            ]

            cities_pattern = '|'.join(all_cities)

            for selector in representative_selectors:
                sections = soup.select(selector)
                for section in sections:
                    section_text = section.get_text()

                    # –ò—â–µ–º –≥–æ—Ä–æ–¥–∞ –≤ —Ç–µ–∫—Å—Ç–µ —Ä–∞–∑–¥–µ–ª–∞
                    for city in all_cities:
                        # –ü–æ–∏—Å–∫ —Ç–æ—á–Ω–æ–≥–æ —É–ø–æ–º–∏–Ω–∞–Ω–∏—è –≥–æ—Ä–æ–¥–∞
                        city_pattern = r'(?:–≥\.|–≥–æ—Ä–æ–¥|–≤)\s*{}'.format(re.escape(city))
                        if re.search(city_pattern, section_text, re.IGNORECASE):
                            address = self._extract_address_from_element(section)
                            representatives.append({
                                "–≥–æ—Ä–æ–¥": city,
                                "–∞–¥—Ä–µ—Å": address,
                                "—Ç–∏–ø": "–ø—Ä–µ–¥—Å—Ç–∞–≤–∏—Ç–µ–ª—å—Å—Ç–≤–æ",
                                "–∏—Å—Ç–æ—á–Ω–∏–∫": "html",
                                "—Å—Ç—Ä–∞–Ω–∞": "–†–æ—Å—Å–∏—è" if city in self.russian_cities else "–ë–µ–ª–∞—Ä—É—Å—å"
                            })

                    # –ò—â–µ–º –Ω–∞–∑–≤–∞–Ω–∏—è –∫–æ–º–ø–∞–Ω–∏–π —Å —É–∫–∞–∑–∞–Ω–∏–µ–º –≥–æ—Ä–æ–¥–æ–≤
                    company_city_pattern = r'(?:–æ–æ–æ|–∑–∞–æ|–∞–æ|–ø–∞–æ)\s+[¬´"]?([^¬ª"]+?)[¬ª"]?(?:\s+[^.!]{0,100}?(?:–≤|–≥\.|–≥–æ—Ä–æ–¥)\s*(' + cities_pattern + '))'
                    company_matches = re.findall(company_city_pattern, section_text, re.IGNORECASE)
                    for company, city in company_matches:
                        exact_city = self._find_exact_city(city, all_cities)
                        if exact_city:
                            representatives.append({
                                "–Ω–∞–∑–≤–∞–Ω–∏–µ_–∫–æ–º–ø–∞–Ω–∏–∏": company.strip(),
                                "–≥–æ—Ä–æ–¥": exact_city,
                                "—Ç–∏–ø": "–∫–æ–º–ø–∞–Ω–∏—è-–ø—Ä–µ–¥—Å—Ç–∞–≤–∏—Ç–µ–ª—å",
                                "–∏—Å—Ç–æ—á–Ω–∏–∫": "html",
                                "—Å—Ç—Ä–∞–Ω–∞": "–†–æ—Å—Å–∏—è" if exact_city in self.russian_cities else "–ë–µ–ª–∞—Ä—É—Å—å"
                            })

        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ HTML –ø—Ä–µ–¥—Å—Ç–∞–≤–∏—Ç–µ–ª—å—Å—Ç–≤: {e}")

        return representatives


    def _extract_address_from_element(self, element) -> str:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∞–¥—Ä–µ—Å–∞ –∏–∑ HTML —ç–ª–µ–º–µ–Ω—Ç–∞"""
        try:
            # –ò—â–µ–º –∞–¥—Ä–µ—Å–Ω—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã
            address_selectors = ['[class*="address"]', '[class*="adress"]', '.address', '.adress']

            for selector in address_selectors:
                address_elem = element.select_one(selector)
                if address_elem:
                    address_text = address_elem.get_text(strip=True)
                    # –û—á–∏—â–∞–µ–º –∞–¥—Ä–µ—Å –æ—Ç –ª–∏—à–Ω–µ–≥–æ
                    clean_address = re.sub(r'(?:—Ç–µ–ª|—Ç–µ–ª–µ—Ñ–æ–Ω|phone|email|@)[^,]*', '', address_text, flags=re.IGNORECASE)
                    clean_address = re.sub(r'\s+', ' ', clean_address).strip()
                    if len(clean_address) > 10:
                        return clean_address

            # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ –ø–æ —Å–µ–ª–µ–∫—Ç–æ—Ä–∞–º, –∏—â–µ–º –≤ —Ç–µ–∫—Å—Ç–µ —ç–ª–µ–º–µ–Ω—Ç–∞
            full_text = element.get_text()
            address_patterns = [
                r'(?:–∞–¥—Ä–µ—Å|address)[:\s]*([–ê-–Ø–∞-—è0-9\s,.-]{10,100}?)(?=[,.]|$)',
                r'[–ê-–Ø–∞-—è0-9\s,.-]{10,100}?(?:—É–ª|—É–ª–∏—Ü–∞|–ø—Ä|–ø—Ä–æ—Å–ø–µ–∫—Ç|–ø–µ—Ä|–ø–µ—Ä–µ—É–ª–æ–∫)[^,.]{5,50}'
            ]

            for pattern in address_patterns:
                matches = re.findall(pattern, full_text, re.IGNORECASE)
                if matches:
                    return matches[0].strip()

        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∞–¥—Ä–µ—Å–∞: {e}")

        return ""

    def extract_contacts_enhanced(self, text: str, html: str = "") -> Dict[str, Any]:
        """–†–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–æ–Ω—Ç–∞–∫—Ç–æ–≤ –≤–∫–ª—é—á–∞—è –ø—Ä–µ–¥—Å—Ç–∞–≤–∏—Ç–µ–ª—å—Å—Ç–≤–∞"""
        contacts = self._extract_contacts(text)

        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ –∏—â–µ–º –∫–æ–Ω—Ç–∞–∫—Ç—ã –≤ HTML
        if html:
            html_contacts = self._extract_contacts_from_html(html)
            # –û–±—ä–µ–¥–∏–Ω—è–µ–º –∫–æ–Ω—Ç–∞–∫—Ç—ã
            contacts["emails"] = list(set(contacts["emails"] + html_contacts["emails"]))[:10]
            contacts["phones"] = list(set(contacts["phones"] + html_contacts["phones"]))[:15]
            contacts["addresses"] = list(set(contacts["addresses"] + html_contacts["addresses"]))[:10]

        return contacts

    def _extract_contacts_from_html(self, html: str) -> Dict[str, List[str]]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–æ–Ω—Ç–∞–∫—Ç–æ–≤ –∏–∑ HTML"""
        contacts = {"emails": [], "phones": [], "addresses": []}

        try:
            soup = BeautifulSoup(html, 'html.parser')

            # –ò—â–µ–º –∫–æ–Ω—Ç–∞–∫—Ç—ã –≤ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã—Ö —ç–ª–µ–º–µ–Ω—Ç–∞—Ö
            contact_selectors = [
                '[class*="contact"]', '[class*="phone"]', '[class*="email"]',
                '[class*="address"]', '.footer', '.contacts'
            ]

            for selector in contact_selectors:
                elements = soup.select(selector)
                for element in elements:
                    text = element.get_text()

                    # Email
                    emails = re.findall(r'\b[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}\b', text)
                    contacts["emails"].extend(emails)

                    # –¢–µ–ª–µ—Ñ–æ–Ω—ã
                    phones = re.findall(r'[\+]?[7|8][\s(-]?\d{3}[\s)-]?\d{3}[\s-]?\d{2}[\s-]?\d{2}', text)
                    contacts["phones"].extend(phones)

                    # –ê–¥—Ä–µ—Å–∞
                    addresses = re.findall(
                        r'(?:–≥\.|–≥–æ—Ä–æ–¥)\s*[–ê-–Ø][–∞-—è]+[^.!?]{0,100}?(?:—É–ª|—É–ª–∏—Ü–∞|–ø—Ä|–ø—Ä–æ—Å–ø–µ–∫—Ç)[^.!?]{10,80}', text)
                    contacts["addresses"].extend(addresses)

        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∫–æ–Ω—Ç–∞–∫—Ç–æ–≤ –∏–∑ HTML: {e}")

        # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã
        for key in contacts:
            contacts[key] = list(set(contacts[key]))

        return contacts

    def _classify_slider_type(self, slider_element, selector: str, text: str) -> str:
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Ç–∏–ø —Å–ª–∞–π–¥–µ—Ä–∞ (–∫–ª–∏–µ–Ω—Ç—ã, –¥–∏–ª–µ—Ä—ã, –ø–∞—Ä—Ç–Ω–µ—Ä—ã)"""
        text_lower = text.lower()
        selector_lower = selector.lower()

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–ª–∞—Å—Å—ã –∏ —Ç–µ–∫—Å—Ç –Ω–∞ –Ω–∞–ª–∏—á–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
        slider_classes = str(slider_element.get('class', [])).lower()

        # –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è –¥–∏–ª–µ—Ä–æ–≤
        dealer_keywords = ['dealer', 'diler', '–¥–∏–ª–µ—Ä', '–¥–∏–ª–ª–µ—Ä', 'dealer-network']
        # –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è –∫–ª–∏–µ–Ω—Ç–æ–≤
        client_keywords = ['client', 'customer', '–∫–ª–∏–µ–Ω—Ç', '–∑–∞–∫–∞–∑—á–∏–∫', 'customer']
        # –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è –ø–∞—Ä—Ç–Ω–µ—Ä–æ–≤
        partner_keywords = ['partner', 'partners', '–ø–∞—Ä—Ç–Ω–µ—Ä']

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–∏–ª–µ—Ä–æ–≤
        if any(keyword in slider_classes or keyword in selector_lower or keyword in text_lower
               for keyword in dealer_keywords):
            return "–¥–∏–ª–µ—Ä—ã"

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–ª–∏–µ–Ω—Ç–æ–≤
        if any(keyword in slider_classes or keyword in selector_lower or keyword in text_lower
               for keyword in client_keywords):
            return "–∫–ª–∏–µ–Ω—Ç—ã"

        # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é —Å—á–∏—Ç–∞–µ–º –ø–∞—Ä—Ç–Ω–µ—Ä–∞–º–∏
        return "–ø–∞—Ä—Ç–Ω–µ—Ä—ã"

    def _extract_companies_from_slider_text(self, text: str) -> List[str]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –Ω–∞–∑–≤–∞–Ω–∏—è –∫–æ–º–ø–∞–Ω–∏–π –∏–∑ —Ç–µ–∫—Å—Ç–∞ —Å–ª–∞–π–¥–µ—Ä–∞"""
        companies = []

        # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è –∫–æ–º–ø–∞–Ω–∏–π –∏–∑ —Ç–µ–∫—Å—Ç–∞
        patterns = [
            r'[¬´"]([^¬ª"]{3,50})[¬ª"]',
            r'\b([–ê-–Ø][A-Z–ê-–Øa-z–∞-—è\s&]{3,30})\b',
            r'\b([A-Z][A-Za-z\s&]{3,30})\b'
        ]

        for pattern in patterns:
            matches = re.findall(pattern, text)
            for match in matches:
                cleaned_name = self._clean_company_name(match)
                if self._looks_like_company_name(cleaned_name):
                    companies.append(cleaned_name)

        return companies

    def _clean_company_name(self, text: str) -> str:
        """–û—á–∏—â–∞–µ—Ç –Ω–∞–∑–≤–∞–Ω–∏–µ –∫–æ–º–ø–∞–Ω–∏–∏ –æ—Ç –º—É—Å–æ—Ä–∞"""
        # –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ —Å–∏–º–≤–æ–ª—ã
        cleaned = re.sub(r'[^\w\s&]', ' ', text)
        # –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã
        cleaned = re.sub(r'\s+', ' ', cleaned)
        # –û–±—Ä–µ–∑–∞–µ–º –ø—Ä–æ–±–µ–ª—ã –ø–æ –∫—Ä–∞—è–º
        cleaned = cleaned.strip()
        return cleaned

    def extract_partners_from_images(self, html: str, base_url: str) -> List[str]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø–∞—Ä—Ç–Ω–µ—Ä–æ–≤ –∏ –∫–ª–∏–µ–Ω—Ç–æ–≤ —Å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —á–µ—Ä–µ–∑ OCR"""
        partners = []

        if not self.reader:
            return partners

        try:
            soup = BeautifulSoup(html, 'html.parser')

            # –ò—â–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –≤ —Ä–∞–∑–¥–µ–ª–∞—Ö –ø–∞—Ä—Ç–Ω–µ—Ä–æ–≤/–∫–ª–∏–µ–Ω—Ç–æ–≤
            partner_selectors = [
                '.partners img', '.clients img', '.customers img',
                '.partners-list img', '.clients-list img',
                'div[class*="partner"] img', 'div[class*="client"] img',
                'section[class*="partner"] img', 'section[class*="client"] img',
                '.dealers img', '.dealer img', '.dilers img'
            ]

            for selector in partner_selectors:
                images = soup.select(selector)
                for img in images[:10]:  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ
                    img_src = img.get('src')
                    if img_src:
                        img_url = urljoin(base_url, img_src)
                        try:
                            # –°–∫–∞—á–∏–≤–∞–µ–º –∏ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
                            response = self.session.get(img_url, timeout=10)
                            if response.status_code == 200:
                                image = Image.open(BytesIO(response.content))

                                # OCR –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞
                                results = self.reader.readtext(image)
                                for (bbox, text, confidence) in results:
                                    if confidence > 0.6 and len(text) > 2:
                                        # –§–∏–ª—å—Ç—Ä—É–µ–º –Ω–∞–∑–≤–∞–Ω–∏—è –∫–æ–º–ø–∞–Ω–∏–π
                                        cleaned_text = self._clean_company_name(text)
                                        if self._looks_like_company_name(cleaned_text):
                                            partners.append(cleaned_text)
                        except Exception as e:
                            continue

        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ OCR –ø–∞—Ä—Ç–Ω–µ—Ä–æ–≤: {e}")

        return list(set(partners))

    def _looks_like_company_name(self, text: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –ø–æ—Ö–æ–∂ –ª–∏ —Ç–µ–∫—Å—Ç –Ω–∞ –Ω–∞–∑–≤–∞–Ω–∏–µ –∫–æ–º–ø–∞–Ω–∏–∏"""
        if len(text) < 3 or len(text) > 50:
            return False

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω—ã –Ω–∞–∑–≤–∞–Ω–∏–π –∫–æ–º–ø–∞–Ω–∏–π
        company_patterns = [
            r'.*[–û–æ][–û–æ][–û–æ].*', r'.*[–ó–∑][–ê–∞][–û–æ].*', r'.*[–ê–∞][–û–æ].*',
            r'.*[–ü–ø][–ê–∞][–û–æ].*', r'.*Inc.*', r'.*Ltd.*', r'.*GmbH.*',
            r'^[–ê-–Ø][A-Z–ê-–Øa-z–∞-—è\s&]+$', r'^[A-Z][A-Za-z\s&]+$'
        ]

        return any(re.match(pattern, text, re.IGNORECASE) for pattern in company_patterns)

    def extract_partners_from_text(self, text: str) -> Dict[str, List[str]]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø–∞—Ä—Ç–Ω–µ—Ä–æ–≤, –∫–ª–∏–µ–Ω—Ç–æ–≤ –∏ –¥–∏–ª–µ—Ä–æ–≤ –∏–∑ —Ç–µ–∫—Å—Ç–∞ —Å —Ä–∞–∑–¥–µ–ª–µ–Ω–∏–µ–º"""
        results = {
            "–ø–∞—Ä—Ç–Ω–µ—Ä—ã": [],
            "–∫–ª–∏–µ–Ω—Ç—ã": [],
            "–¥–∏–ª–µ—Ä—ã": []
        }

        # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –ø–∞—Ä—Ç–Ω–µ—Ä–æ–≤
        partner_patterns = [
            r'(?:–ø–∞—Ä—Ç–Ω–µ—Ä—ã?|—Å–æ—Ç—Ä—É–¥–Ω–∏—á–∞–µ–º)(?:\s*[:‚Äî]\s*)([^.!?]+)',
            r'–Ω–∞—à–∏\s+–ø–∞—Ä—Ç–Ω–µ—Ä—ã(?:\s*[.:]?\s*)([^.!?]+)',
            r'–ø–∞—Ä—Ç–Ω–µ—Ä—Å–∫–∞—è\s+—Å–µ—Ç—å[^.!?]*?([^.!?]+)',
        ]

        # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –∫–ª–∏–µ–Ω—Ç–æ–≤
        client_patterns = [
            r'(?:–∫–ª–∏–µ–Ω—Ç—ã?|–∑–∞–∫–∞–∑—á–∏–∫–∏?)(?:\s*[:‚Äî]\s*)([^.!?]+)',
            r'–Ω–∞—à–∏\s+–∫–ª–∏–µ–Ω—Ç—ã(?:\s*[.:]?\s*)([^.!?]+)',
            r'—Ä–∞–±–æ—Ç–∞–µ–º\s+—Å[^.!?]*?([^.!?]+)',
        ]

        # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –¥–∏–ª–µ—Ä–æ–≤
        dealer_patterns = [
            r'(?:–¥–∏–ª–µ—Ä—ã?|–¥–∏–ª–µ—Ä—Å–∫–∞—è\s+—Å–µ—Ç—å)(?:\s*[:‚Äî]\s*)([^.!?]+)',
            r'–æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–µ\s+–¥–∏–ª–µ—Ä—ã(?:\s*[.:]?\s*)([^.!?]+)',
            r'—Å–µ—Ç—å\s+–¥–∏–ª–µ—Ä–æ–≤[^.!?]*?([^.!?]+)',
        ]

        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –ø–∞—Ä—Ç–Ω–µ—Ä–æ–≤
        for pattern in partner_patterns:
            matches = re.findall(pattern, text, re.IGNORECASE | re.MULTILINE)
            for match in matches:
                companies = self._extract_companies_from_context(match)
                results["–ø–∞—Ä—Ç–Ω–µ—Ä—ã"].extend(companies)

        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–ª–∏–µ–Ω—Ç–æ–≤
        for pattern in client_patterns:
            matches = re.findall(pattern, text, re.IGNORECASE | re.MULTILINE)
            for match in matches:
                companies = self._extract_companies_from_context(match)
                results["–∫–ª–∏–µ–Ω—Ç—ã"].extend(companies)

        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –¥–∏–ª–µ—Ä–æ–≤
        for pattern in dealer_patterns:
            matches = re.findall(pattern, text, re.IGNORECASE | re.MULTILINE)
            for match in matches:
                companies = self._extract_companies_from_context(match)
                results["–¥–∏–ª–µ—Ä—ã"].extend(companies)

        # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –∏ –æ—á–∏—Å—Ç–∫–∞
        for category in results:
            cleaned_companies = []
            for company in set(results[category]):
                company = company.strip()
                if (len(company) >= 4 and
                        not any(word in company.lower() for word in ['–∫–æ–º–ø–∞–Ω–∏—è', '–æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏—è', '—Ñ–∏—Ä–º–∞']) and
                        self._looks_like_company_name(company)):
                    cleaned_companies.append(company)
            results[category] = cleaned_companies[:20]  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ

        return results

    def _extract_companies_from_context(self, context: str) -> List[str]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –Ω–∞–∑–≤–∞–Ω–∏–π –∫–æ–º–ø–∞–Ω–∏–π –∏–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞"""
        companies = []

        # –ü–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –Ω–∞–∑–≤–∞–Ω–∏–π –∫–æ–º–ø–∞–Ω–∏–π
        company_patterns = [
            r'[¬´"]([^¬ª"]+?)[¬ª"]',  # –í –∫–∞–≤—ã—á–∫–∞—Ö
            r'(?:–û–û–û|–ó–ê–û|–ê–û|–ü–ê–û|–ò–ü)\s+[¬´"]?([^¬ª".!?,]+)',  # –° —É–∫–∞–∑–∞–Ω–∏–µ–º —Ñ–æ—Ä–º—ã
            r'([–ê-–Ø][A-Z–ê-–Øa-z–∞-—è\s&]+?(?:–û–û–û|–ó–ê–û|–ê–û|–ü–ê–û|Inc|Ltd))',  # –° –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä–æ–π
            r'([–ê-–Ø][A-Z–ê-–Øa-z–∞-—è\s&]{3,})'  # –õ—é–±–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ —Å –∑–∞–≥–ª–∞–≤–Ω–æ–π –±—É–∫–≤—ã
        ]

        for pattern in company_patterns:
            matches = re.findall(pattern, context)
            companies.extend(matches)

        return companies

    def extract_management_reliable(self, text: str, url: str = "") -> List[Dict[str, Any]]:
        """–ù–∞–¥–µ–∂–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–∞ —Å –ø–æ–º–æ—â—å—é —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤"""
        management = []

        # –°—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏ —á–∞—Å—Ç—ã–µ —Å–æ—á–µ—Ç–∞–Ω–∏—è –¥–ª—è –≥–µ–Ω–µ—Ä–∞–ª—å–Ω–æ–≥–æ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∞
        director_patterns = [
            r'(–≥–µ–Ω–µ—Ä–∞–ª—å–Ω—ã–π\s+–¥–∏—Ä–µ–∫—Ç–æ—Ä|–≥–µ–Ω–¥–∏—Ä–µ–∫—Ç–æ—Ä|–¥–∏—Ä–µ–∫—Ç–æ—Ä|–ø—Ä–µ–¥—Å–µ–¥–∞—Ç–µ–ª—å)(?:\s+[‚Äî:-]\s*)?([–ê-–Ø][–∞-—è]+\s+[–ê-–Ø][–∞-—è]+(?:\s+[–ê-–Ø][–∞-—è]+)?)',
            r'([–ê-–Ø][–∞-—è]+\s+[–ê-–Ø][–∞-—è]+(?:\s+[–ê-–Ø][–∞-—è]+)?)(?:\s+[‚Äî:-]\s*)?(–≥–µ–Ω–µ—Ä–∞–ª—å–Ω—ã–π\s+–¥–∏—Ä–µ–∫—Ç–æ—Ä|–≥–µ–Ω–¥–∏—Ä–µ–∫—Ç–æ—Ä|–¥–∏—Ä–µ–∫—Ç–æ—Ä)',
            r'(–¥–∏—Ä–µ–∫—Ç–æ—Ä|—Ä—É–∫–æ–≤–æ–¥–∏—Ç–µ–ª—å|–ø—Ä–µ–¥—Å–µ–¥–∞—Ç–µ–ª—å)(?:\s+[‚Äî:-]\s*)?([–ê-–Ø][–∞-—è]+\s+[–ê-–Ø][–∞-—è]+)',
            r'([–ê-–Ø][–∞-—è]+\s+[–ê-–Ø][–∞-—è]+)(?:\s+[‚Äî:-]\s*)?(–¥–∏—Ä–µ–∫—Ç–æ—Ä|—Ä—É–∫–æ–≤–æ–¥–∏—Ç–µ–ª—å|–ø—Ä–µ–¥—Å–µ–¥–∞—Ç–µ–ª—å)'
        ]

        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã
        context_patterns = [
            r'(?:–≤–æ\s+–≥–ª–∞–≤–µ\s+[^.!?]+?\s+—Å—Ç–æ–∏—Ç\s+)([–ê-–Ø][–∞-—è]+\s+[–ê-–Ø][–∞-—è]+)',
            r'(?:—Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ\s+[^.!?]+?\s+–æ—Å—É—â–µ—Å—Ç–≤–ª—è–µ—Ç\s+)([–ê-–Ø][–∞-—è]+\s+[–ê-–Ø][–∞-—è]+)',
            r'(?:–¥–∏—Ä–µ–∫—Ç–æ—Ä–æ–º\s+[^.!?]+?\s+—è–≤–ª—è–µ—Ç—Å—è\s+)([–ê-–Ø][–∞-—è]+\s+[–ê-–Ø][–∞-—è]+)'
        ]

        # –ü–æ–∏—Å–∫ –ø–æ –æ—Å–Ω–æ–≤–Ω—ã–º –ø–∞—Ç—Ç–µ—Ä–Ω–∞–º
        for pattern in director_patterns:
            matches = re.findall(pattern, text, re.IGNORECASE)
            for match in matches:
                if len(match) == 2:
                    position, name = match
                    if self._is_valid_russian_name(name):
                        management.append({
                            "–¥–æ–ª–∂–Ω–æ—Å—Ç—å": position.strip().title(),
                            "–∏–º—è": name.strip(),
                            "–∏—Å—Ç–æ—á–Ω–∏–∫": "–ø–∞—Ç—Ç–µ—Ä–Ω",
                            "–∫–æ–Ω—Ç–µ–∫—Å—Ç": self._extract_context(text, name)
                        })
                    elif self._is_valid_russian_name(position):
                        management.append({
                            "–¥–æ–ª–∂–Ω–æ—Å—Ç—å": name.strip().title(),
                            "–∏–º—è": position.strip(),
                            "–∏—Å—Ç–æ—á–Ω–∏–∫": "–ø–∞—Ç—Ç–µ—Ä–Ω",
                            "–∫–æ–Ω—Ç–µ–∫—Å—Ç": self._extract_context(text, position)
                        })

        # –ü–æ–∏—Å–∫ –ø–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–º –ø–∞—Ç—Ç–µ—Ä–Ω–∞–º
        for pattern in context_patterns:
            matches = re.findall(pattern, text, re.IGNORECASE)
            for name in matches:
                if self._is_valid_russian_name(name):
                    management.append({
                        "–¥–æ–ª–∂–Ω–æ—Å—Ç—å": "–ì–µ–Ω–µ—Ä–∞–ª—å–Ω—ã–π –¥–∏—Ä–µ–∫—Ç–æ—Ä",
                        "–∏–º—è": name.strip(),
                        "–∏—Å—Ç–æ—á–Ω–∏–∫": "–∫–æ–Ω—Ç–µ–∫—Å—Ç",
                        "–∫–æ–Ω—Ç–µ–∫—Å—Ç": self._extract_context(text, name)
                    })

        # –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã
        unique_management = []
        seen_names = set()
        for person in management:
            if person['–∏–º—è'] not in seen_names:
                unique_management.append(person)
                seen_names.add(person['–∏–º—è'])

        print(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(unique_management)} —Ä—É–∫–æ–≤–æ–¥–∏—Ç–µ–ª–µ–π")
        return unique_management

    def _is_valid_russian_name(self, name: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –≤–∞–ª–∏–¥–Ω–æ—Å—Ç—å —Ä—É—Å—Å–∫–æ–≥–æ –∏–º–µ–Ω–∏"""
        name_parts = name.split()
        if len(name_parts) not in [2, 3]:
            return False

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä—É—Å—Å–∫–∏–µ —Å–∏–º–≤–æ–ª—ã –∏ –∑–∞–≥–ª–∞–≤–Ω—ã–µ –±—É–∫–≤—ã
        russian_chars = set('–∞–±–≤–≥–¥–µ—ë–∂–∑–∏–π–∫–ª–º–Ω–æ–ø—Ä—Å—Ç—É—Ñ—Ö—Ü—á—à—â—ä—ã—å—ç—é—è')
        for part in name_parts:
            if not part.istitle() or len(part) < 2:
                return False
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ —Ä—É—Å—Å–∫–∏—Ö —Å–∏–º–≤–æ–ª–æ–≤
            if not any(char.lower() in russian_chars for char in part):
                return False

        return True

    def _extract_context(self, text: str, target: str, context_size: int = 100) -> str:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç –≤–æ–∫—Ä—É–≥ —Ü–µ–ª–µ–≤–æ–π —Ñ—Ä–∞–∑—ã"""
        start_idx = text.find(target)
        if start_idx == -1:
            return ""

        end_idx = start_idx + len(target)
        context_start = max(0, start_idx - context_size)
        context_end = min(len(text), end_idx + context_size)

        return text[context_start:context_end].strip()

    def crawl_priority_pages(self, base_url: str) -> Dict[str, str]:
        """–ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω—ã–π –æ–±—Ö–æ–¥ –∫–ª—é—á–µ–≤—ã—Ö —Å—Ç—Ä–∞–Ω–∏—Ü"""
        print(f"üîç –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω—ã–π –ø–æ–∏—Å–∫ –∫–ª—é—á–µ–≤—ã—Ö —Å—Ç—Ä–∞–Ω–∏—Ü: {base_url}")

        all_pages_text = {}
        priority_urls = self._generate_priority_urls(base_url)

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω—ã—Ö —Å—Ç—Ä–∞–Ω–∏—Ü
        valid_urls = []
        for url in priority_urls:
            if self._check_url_access(url):
                valid_urls.append(url)
                print(f"‚úÖ –ù–∞–π–¥–µ–Ω–∞: {url}")

        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –Ω–∞–π–¥–µ–Ω–Ω—ã–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
        print(f"üöÄ –û–±—Ä–∞–±–æ—Ç–∫–∞ {len(valid_urls)} –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω—ã—Ö —Å—Ç—Ä–∞–Ω–∏—Ü...")
        for url in valid_urls[:10]:  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ
            try:
                page_text = self._process_single_page(url)
                if page_text:
                    all_pages_text[url] = page_text
            except Exception as e:
                print(f"‚ùå –û—à–∏–±–∫–∞ {url}: {e}")

        return all_pages_text

    def _generate_priority_urls(self, base_url: str) -> List[str]:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω—ã—Ö URL"""
        priority_urls = []
        base_domain = urlparse(base_url).netloc
        base_scheme = urlparse(base_url).scheme

        for path in self.priority_paths:
            # –ë–µ–∑ www
            priority_urls.append(f"{base_scheme}://{base_domain}{path}")
            # –° www
            priority_urls.append(f"{base_scheme}://www.{base_domain}{path}")

        return priority_urls

    def _check_url_access(self, url: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ URL"""
        try:
            response = self.session.head(url, timeout=5, allow_redirects=True)
            return response.status_code == 200
        except:
            return False

    def _process_single_page(self, url: str) -> str:
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –æ–¥–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã"""
        try:
            response = self.session.get(url, timeout=10)
            response.encoding = 'utf-8'

            if response.status_code == 200:
                return self._extract_text_from_html(response.text, url)
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ {url}: {e}")

        return ""

    def _extract_text_from_html(self, html: str, url: str = "") -> str:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —á–∏—Å—Ç–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ –∏–∑ HTML"""
        soup = BeautifulSoup(html, 'html.parser')

        # –£–¥–∞–ª—è–µ–º –Ω–µ–Ω—É–∂–Ω—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã
        for element in soup(['script', 'style', 'nav', 'footer', 'header', 'meta', 'link']):
            element.decompose()

        # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω—ã–µ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã –¥–ª—è about/team —Å—Ç—Ä–∞–Ω–∏—Ü
        content_selectors = [
            'main', 'article', 'section',
            '.content', '.main-content', '.page-content',
            '.about-content', '.company-info', '.team-section',
            '.management-list', '.executive-team', '.directors',
            '.contact-info', '.contacts-list',
            '.partners', '.clients', '.dealers'
        ]

        text_parts = []

        # –ò—â–µ–º –ø–æ —Å–ø–µ—Ü–∏—Ñ–∏—á–µ—Å–∫–∏–º —Å–µ–ª–µ–∫—Ç–æ—Ä–∞–º
        for selector in content_selectors:
            elements = soup.select(selector)
            for element in elements:
                text = element.get_text(strip=True)
                if len(text) > 50:
                    text_parts.append(text)

        # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ –ø–æ —Å–µ–ª–µ–∫—Ç–æ—Ä–∞–º, –±–µ—Ä–µ–º –æ—Å–Ω–æ–≤–Ω–æ–π –∫–æ–Ω—Ç–µ–Ω—Ç
        if not text_parts:
            content_elements = soup.find_all(['p', 'div', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6'])
            for element in content_elements:
                text = element.get_text(strip=True)
                if len(text) > 30:
                    text_parts.append(text)

        full_text = ' '.join(text_parts)
        full_text = re.sub(r'\s+', ' ', full_text)

        return full_text[:100000]  # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –ª–∏–º–∏—Ç –¥–ª—è about-—Å—Ç—Ä–∞–Ω–∏—Ü

    def comprehensive_analysis(self, base_url: str) -> Dict[str, Any]:
        """–ö–æ–º–ø–ª–µ–∫—Å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –∫–æ–º–ø–∞–Ω–∏–∏"""
        print(f"\nüéØ –ö–û–ú–ü–õ–ï–ö–°–ù–´–ô –ê–ù–ê–õ–ò–ó: {base_url}")

        # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω—ã–π –æ–±—Ö–æ–¥ –∫–ª—é—á–µ–≤—ã—Ö —Å—Ç—Ä–∞–Ω–∏—Ü
        priority_pages = self.crawl_priority_pages(base_url)

        if not priority_pages:
            return {"error": "–ù–µ –Ω–∞–π–¥–µ–Ω—ã –∫–ª—é—á–µ–≤—ã–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞"}

        print(f"üìä –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {len(priority_pages)} –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω—ã—Ö —Å—Ç—Ä–∞–Ω–∏—Ü")

        # –ê–Ω–∞–ª–∏–∑ –∫–∞–∂–¥–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã –æ—Ç–¥–µ–ª—å–Ω–æ
        all_partners_data = {
            "—Ç–µ–∫—Å—Ç_–ø–∞—Ä—Ç–Ω–µ—Ä—ã": [],
            "—Ç–µ–∫—Å—Ç_–∫–ª–∏–µ–Ω—Ç—ã": [],
            "—Ç–µ–∫—Å—Ç_–¥–∏–ª–µ—Ä—ã": [],
            "–∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è_–ø–∞—Ä—Ç–Ω–µ—Ä—ã": [],
            "—Å–ª–∞–π–¥–µ—Ä_–∫–ª–∏–µ–Ω—Ç—ã": [],
            "—Å–ª–∞–π–¥–µ—Ä_–¥–∏–ª–µ—Ä—ã": [],
            "—Å–ª–∞–π–¥–µ—Ä_–ø–∞—Ä—Ç–Ω–µ—Ä—ã": []
        }
        all_management = []
        all_contacts = []
        all_representatives = []

        for url, text in priority_pages.items():
            print(f"\nüìÑ –ê–Ω–∞–ª–∏–∑ —Å—Ç—Ä–∞–Ω–∏—Ü—ã: {url}")

            try:
                # –ü–æ–ª—É—á–∞–µ–º HTML –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —Å–ª–∞–π–¥–µ—Ä–æ–≤
                response = self.session.get(url, timeout=10)
                html_content = response.text

                # –ê–Ω–∞–ª–∏–∑ —Å–ª–∞–π–¥–µ—Ä–æ–≤
                slider_data = self.extract_slider_clients(html_content, base_url)
                all_partners_data["—Å–ª–∞–π–¥–µ—Ä_–∫–ª–∏–µ–Ω—Ç—ã"].extend(slider_data["–∫–ª–∏–µ–Ω—Ç—ã"])
                all_partners_data["—Å–ª–∞–π–¥–µ—Ä_–¥–∏–ª–µ—Ä—ã"].extend(slider_data["–¥–∏–ª–µ—Ä—ã"])
                all_partners_data["—Å–ª–∞–π–¥–µ—Ä_–ø–∞—Ä—Ç–Ω–µ—Ä—ã"].extend(slider_data["–ø–∞—Ä—Ç–Ω–µ—Ä—ã"])

                # –ü–∞—Ä—Ç–Ω–µ—Ä—ã –∏–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
                image_partners = self.extract_partners_from_images(html_content, base_url)
                all_partners_data["–∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è_–ø–∞—Ä—Ç–Ω–µ—Ä—ã"].extend(image_partners)

                # –ê–Ω–∞–ª–∏–∑ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏—Ç–µ–ª—å—Å—Ç–≤ (–æ—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥)
                page_representatives = self.extract_representatives(text, html_content)
                all_representatives.extend(page_representatives)

                # –ù–û–í–û–ï: –ê–Ω–∞–ª–∏–∑ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏—Ç–µ–ª—å—Å—Ç–≤ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–∞—Ö –∫–æ–Ω—Ç–∞–∫—Ç–æ–≤/–¥–∏–ª–µ—Ä–æ–≤
                contacts_representatives = self.extract_representatives_from_contacts_pages(text, html_content, url)
                all_representatives.extend(contacts_representatives)

            except Exception as e:
                print(f"‚ùå –û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ HTML: {e}")

            # –ü–∞—Ä—Ç–Ω–µ—Ä—ã –∏–∑ —Ç–µ–∫—Å—Ç–∞
            text_partners_data = self.extract_partners_from_text(text)
            all_partners_data["—Ç–µ–∫—Å—Ç_–ø–∞—Ä—Ç–Ω–µ—Ä—ã"].extend(text_partners_data["–ø–∞—Ä—Ç–Ω–µ—Ä—ã"])
            all_partners_data["—Ç–µ–∫—Å—Ç_–∫–ª–∏–µ–Ω—Ç—ã"].extend(text_partners_data["–∫–ª–∏–µ–Ω—Ç—ã"])
            all_partners_data["—Ç–µ–∫—Å—Ç_–¥–∏–ª–µ—Ä—ã"].extend(text_partners_data["–¥–∏–ª–µ—Ä—ã"])

            # –†—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ
            page_management = self.extract_management_reliable(text, url)
            all_management.extend(page_management)

            # –ö–æ–Ω—Ç–∞–∫—Ç—ã
            contacts = self._extract_contacts(text)
            if contacts["emails"] or contacts["phones"]:
                all_contacts.append({
                    "url": url,
                    "contacts": contacts
                })

        # –£–±–∏—Ä–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã –∏ —Ñ–æ—Ä–º–∏—Ä—É–µ–º –∏—Ç–æ–≥–æ–≤—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        final_partners_data = {}
        for key, companies in all_partners_data.items():
            final_partners_data[key] = list(set(companies))

        unique_management = self._remove_duplicate_management(all_management)

        # –û–±—Ä–∞–±–æ—Ç–∫–∞ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏—Ç–µ–ª—å—Å—Ç–≤
        unique_representatives = []
        seen_repr = set()
        for repr in all_representatives:
            key = f"{repr.get('–≥–æ—Ä–æ–¥', '')}_{repr.get('–Ω–∞–∑–≤–∞–Ω–∏–µ_–∫–æ–º–ø–∞–Ω–∏–∏', '')}_{repr.get('–∞–¥—Ä–µ—Å', '')}"
            if key not in seen_repr and key != "__":
                unique_representatives.append(repr)
                seen_repr.add(key)

        # –û–ë–™–ï–î–ò–ù–ï–ù–ò–ï –í–°–ï–• –î–ê–ù–ù–´–• –í –ï–î–ò–ù–´–ï –ö–ê–¢–ï–ì–û–†–ò–ò
        consolidated_partners = {
            "–≤—Å–µ_–ø–∞—Ä—Ç–Ω–µ—Ä—ã": list(set(
                final_partners_data["—Ç–µ–∫—Å—Ç_–ø–∞—Ä—Ç–Ω–µ—Ä—ã"] +
                final_partners_data["–∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è_–ø–∞—Ä—Ç–Ω–µ—Ä—ã"] +
                final_partners_data["—Å–ª–∞–π–¥–µ—Ä_–ø–∞—Ä—Ç–Ω–µ—Ä—ã"]
            )),
            "–≤—Å–µ_–∫–ª–∏–µ–Ω—Ç—ã": list(set(
                final_partners_data["—Ç–µ–∫—Å—Ç_–∫–ª–∏–µ–Ω—Ç—ã"] +
                final_partners_data["—Å–ª–∞–π–¥–µ—Ä_–∫–ª–∏–µ–Ω—Ç—ã"]
            )),
            "–≤—Å–µ_–¥–∏–ª–µ—Ä—ã": list(set(
                final_partners_data["—Ç–µ–∫—Å—Ç_–¥–∏–ª–µ—Ä—ã"] +
                final_partners_data["—Å–ª–∞–π–¥–µ—Ä_–¥–∏–ª–µ—Ä—ã"]
            ))
        }

        # –§–æ—Ä–º–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        result = {
            "url": base_url,
            "—Å—Ç–∞—Ç—É—Å": "—É—Å–ø–µ—à–Ω–æ",
            "—Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ": unique_management,
            "–±–∏–∑–Ω–µ—Å_–ø–∞—Ä—Ç–Ω–µ—Ä—ã": {
                "–¥–µ—Ç–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ_–¥–∞–Ω–Ω—ã–µ": final_partners_data,
                "–æ–±—ä–µ–¥–∏–Ω–µ–Ω–Ω—ã–µ_–¥–∞–Ω–Ω—ã–µ": consolidated_partners
            },
            "–∫–æ–Ω—Ç–∞–∫—Ç—ã": all_contacts,
            "–ø—Ä–µ–¥—Å—Ç–∞–≤–∏—Ç–µ–ª—å—Å—Ç–≤–∞": unique_representatives,
            "–ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ_—Å—Ç—Ä–∞–Ω–∏—Ü—ã": list(priority_pages.keys()),
            "–º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ": {
                "–Ω–∞–π–¥–µ–Ω–æ_—Ä—É–∫–æ–≤–æ–¥–∏—Ç–µ–ª–µ–π": len(unique_management),
                # –î–µ—Ç–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
                "–Ω–∞–π–¥–µ–Ω–æ_–ø–∞—Ä—Ç–Ω–µ—Ä–æ–≤_—Ç–µ–∫—Å—Ç": len(final_partners_data["—Ç–µ–∫—Å—Ç_–ø–∞—Ä—Ç–Ω–µ—Ä—ã"]),
                "–Ω–∞–π–¥–µ–Ω–æ_–∫–ª–∏–µ–Ω—Ç–æ–≤_—Ç–µ–∫—Å—Ç": len(final_partners_data["—Ç–µ–∫—Å—Ç_–∫–ª–∏–µ–Ω—Ç—ã"]),
                "–Ω–∞–π–¥–µ–Ω–æ_–¥–∏–ª–µ—Ä–æ–≤_—Ç–µ–∫—Å—Ç": len(final_partners_data["—Ç–µ–∫—Å—Ç_–¥–∏–ª–µ—Ä—ã"]),
                "–Ω–∞–π–¥–µ–Ω–æ_–ø–∞—Ä—Ç–Ω–µ—Ä–æ–≤_–∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è": len(final_partners_data["–∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è_–ø–∞—Ä—Ç–Ω–µ—Ä—ã"]),
                "–Ω–∞–π–¥–µ–Ω–æ_–∫–ª–∏–µ–Ω—Ç–æ–≤_—Å–ª–∞–π–¥–µ—Ä": len(final_partners_data["—Å–ª–∞–π–¥–µ—Ä_–∫–ª–∏–µ–Ω—Ç—ã"]),
                "–Ω–∞–π–¥–µ–Ω–æ_–¥–∏–ª–µ—Ä–æ–≤_—Å–ª–∞–π–¥–µ—Ä": len(final_partners_data["—Å–ª–∞–π–¥–µ—Ä_–¥–∏–ª–µ—Ä—ã"]),
                "–Ω–∞–π–¥–µ–Ω–æ_–ø–∞—Ä—Ç–Ω–µ—Ä–æ–≤_—Å–ª–∞–π–¥–µ—Ä": len(final_partners_data["—Å–ª–∞–π–¥–µ—Ä_–ø–∞—Ä—Ç–Ω–µ—Ä—ã"]),
                # –û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
                "–≤—Å–µ–≥–æ_–ø–∞—Ä—Ç–Ω–µ—Ä–æ–≤": len(consolidated_partners["–≤—Å–µ_–ø–∞—Ä—Ç–Ω–µ—Ä—ã"]),
                "–≤—Å–µ–≥–æ_–∫–ª–∏–µ–Ω—Ç–æ–≤": len(consolidated_partners["–≤—Å–µ_–∫–ª–∏–µ–Ω—Ç—ã"]),
                "–≤—Å–µ–≥–æ_–¥–∏–ª–µ—Ä–æ–≤": len(consolidated_partners["–≤—Å–µ_–¥–∏–ª–µ—Ä—ã"]),
                "–Ω–∞–π–¥–µ–Ω–æ_—Å—Ç—Ä–∞–Ω–∏—Ü_—Å_–∫–æ–Ω—Ç–∞–∫—Ç–∞–º–∏": len(all_contacts),
                "–Ω–∞–π–¥–µ–Ω–æ_–ø—Ä–µ–¥—Å—Ç–∞–≤–∏—Ç–µ–ª—å—Å—Ç–≤": len(unique_representatives)
            }
        }

        return result

    def _remove_duplicate_management(self, management: List[Dict]) -> List[Dict]:
        """–£–¥–∞–ª—è–µ—Ç –¥—É–±–ª–∏–∫–∞—Ç—ã –≤ —Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–µ"""
        unique = []
        seen_names = set()

        for person in management:
            if person['–∏–º—è'] not in seen_names:
                unique.append(person)
                seen_names.add(person['–∏–º—è'])

        return unique

    def _extract_contacts(self, text: str) -> Dict[str, List[str]]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–æ–Ω—Ç–∞–∫—Ç–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏"""
        contacts = {"emails": [], "phones": [], "addresses": []}

        # Email
        email_pattern = r'\b[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}\b'
        emails = re.findall(email_pattern, text)
        contacts["emails"] = list(set(emails))[:5]

        # –¢–µ–ª–µ—Ñ–æ–Ω—ã
        phone_patterns = [
            r'[\+]?[7|8][\s(-]?\d{3}[\s)-]?\d{3}[\s-]?\d{2}[\s-]?\d{2}',
            r'\(\d{3,4}\)\s?\d{2,3}[\s-]?\d{2}[\s-]?\d{2}',
        ]

        all_phones = []
        for pattern in phone_patterns:
            phones = re.findall(pattern, text)
            all_phones.extend(phones)

        contacts["phones"] = list(set(all_phones))[:10]

        return contacts


def main():
    """–î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è —Ä–∞–±–æ—Ç—ã —É–ª—É—á—à–µ–Ω–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞"""

    print("üîÑ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è EnhancedCompanySiteAnalyzer...")
    analyzer = EnhancedCompanySiteAnalyzer(max_pages=15, max_depth=1)

    # –¢–µ—Å—Ç–æ–≤—ã–µ —Å–∞–π—Ç—ã
    test_urls = [
        "https://privod.ru/",
        "https://www.prst.ru/",
        "https://optimusdrive.ru/"
    ]

    for site in test_urls:
        print(f"\n{'=' * 80}")
        print(f"üîç –ê–ù–ê–õ–ò–ó: {site}")
        print(f"{'=' * 80}")

        try:
            result = analyzer.comprehensive_analysis(site)

            if "error" in result:
                print(f"‚ùå –û—à–∏–±–∫–∞: {result['error']}")
            else:
                print("‚úÖ –ê–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω —É—Å–ø–µ—à–Ω–æ!")

                # –í—ã–≤–æ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
                management = result.get("—Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ", [])
                partners_data = result.get("–±–∏–∑–Ω–µ—Å_–ø–∞—Ä—Ç–Ω–µ—Ä—ã", {})
                consolidated_data = partners_data.get("–æ–±—ä–µ–¥–∏–Ω–µ–Ω–Ω—ã–µ_–¥–∞–Ω–Ω—ã–µ", {})
                representatives = result.get("–ø—Ä–µ–¥—Å—Ç–∞–≤–∏—Ç–µ–ª—å—Å—Ç–≤–∞", [])  # –ù–û–í–û–ï: –ø—Ä–µ–¥—Å—Ç–∞–≤–∏—Ç–µ–ª—å—Å—Ç–≤–∞
                meta = result.get("–º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ", {})

                print(f"\nüëë –†–£–ö–û–í–û–î–°–¢–í–û:")
                if management:
                    for i, person in enumerate(management, 1):
                        print(f"   {i}. {person['–¥–æ–ª–∂–Ω–æ—Å—Ç—å']}: {person['–∏–º—è']}")
                else:
                    print("   ‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω–æ")

                # –í–´–í–û–î –û–ë–™–ï–î–ò–ù–ï–ù–ù–´–• –î–ê–ù–ù–´–•
                print(f"\nü§ù –í–°–ï –ü–ê–†–¢–ù–ï–†–´ ({len(consolidated_data.get('–≤—Å–µ_–ø–∞—Ä—Ç–Ω–µ—Ä—ã', []))}):")
                all_partners = consolidated_data.get("–≤—Å–µ_–ø–∞—Ä—Ç–Ω–µ—Ä—ã", [])
                if all_partners:
                    for i, partner in enumerate(all_partners[:15], 1):
                        print(f"   {i}. {partner}")
                    if len(all_partners) > 15:
                        print(f"   ... –∏ –µ—â–µ {len(all_partners) - 15}")
                else:
                    print("   ‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω–æ")

                print(f"\nüíº –í–°–ï –ö–õ–ò–ï–ù–¢–´ ({len(consolidated_data.get('–≤—Å–µ_–∫–ª–∏–µ–Ω—Ç—ã', []))}):")
                all_clients = consolidated_data.get("–≤—Å–µ_–∫–ª–∏–µ–Ω—Ç—ã", [])
                if all_clients:
                    for i, client in enumerate(all_clients[:15], 1):
                        print(f"   {i}. {client}")
                    if len(all_clients) > 15:
                        print(f"   ... –∏ –µ—â–µ {len(all_clients) - 15}")
                else:
                    print("   ‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω–æ")

                print(f"\nüöó –í–°–ï –î–ò–õ–ï–†–´ ({len(consolidated_data.get('–≤—Å–µ_–¥–∏–ª–µ—Ä—ã', []))}):")
                all_dealers = consolidated_data.get("–≤—Å–µ_–¥–∏–ª–µ—Ä—ã", [])
                if all_dealers:
                    for i, dealer in enumerate(all_dealers[:15], 1):
                        print(f"   {i}. {dealer}")
                    if len(all_dealers) > 15:
                        print(f"   ... –∏ –µ—â–µ {len(all_dealers) - 15}")
                else:
                    print("   ‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω–æ")

                # –ù–û–í–û–ï: –í–´–í–û–î –ü–†–ï–î–°–¢–ê–í–ò–¢–ï–õ–¨–°–¢–í
                print(f"\nüè¢ –ü–†–ï–î–°–¢–ê–í–ò–¢–ï–õ–¨–°–¢–í–ê ({len(representatives)}):")
                if representatives:
                    for i, repr in enumerate(representatives, 1):
                        if repr.get('–≥–æ—Ä–æ–¥'):
                            country_flag = "üá∑üá∫" if repr.get('—Å—Ç—Ä–∞–Ω–∞') == '–†–æ—Å—Å–∏—è' else "üáßüáæ" if repr.get('—Å—Ç—Ä–∞–Ω–∞') == '–ë–µ–ª–∞—Ä—É—Å—å' else "üè¢"
                            print(f"   {i}. {country_flag} {repr['–≥–æ—Ä–æ–¥']}", end="")
                            if repr.get('–∞–¥—Ä–µ—Å'):
                                print(f" - {repr['–∞–¥—Ä–µ—Å']}")
                            else:
                                print()
                        elif repr.get('–Ω–∞–∑–≤–∞–Ω–∏–µ_–∫–æ–º–ø–∞–Ω–∏–∏'):
                            print(f"   {i}. üè¢ {repr['–Ω–∞–∑–≤–∞–Ω–∏–µ_–∫–æ–º–ø–∞–Ω–∏–∏']}", end="")
                            if repr.get('—Å–∞–π—Ç'):
                                print(f" | üåê {repr['—Å–∞–π—Ç']}", end="")
                            if repr.get('–∞–¥—Ä–µ—Å'):
                                print(f" | üìç {repr['–∞–¥—Ä–µ—Å']}", end="")
                            if repr.get('—Ç–µ–ª–µ—Ñ–æ–Ω'):
                                print(f" | üìû {repr['—Ç–µ–ª–µ—Ñ–æ–Ω']}", end="")
                            print()
                else:
                    print("   ‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω–æ")

                print(f"\nüìä –û–ë–©–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê:")
                print(f"   üëë –†—É–∫–æ–≤–æ–¥–∏—Ç–µ–ª–µ–π: {meta.get('–Ω–∞–π–¥–µ–Ω–æ_—Ä—É–∫–æ–≤–æ–¥–∏—Ç–µ–ª–µ–π', 0)}")
                print(f"   ü§ù –í—Å–µ–≥–æ –ø–∞—Ä—Ç–Ω–µ—Ä–æ–≤: {meta.get('–≤—Å–µ–≥–æ_–ø–∞—Ä—Ç–Ω–µ—Ä–æ–≤', 0)}")
                print(f"   üíº –í—Å–µ–≥–æ –∫–ª–∏–µ–Ω—Ç–æ–≤: {meta.get('–≤—Å–µ–≥–æ_–∫–ª–∏–µ–Ω—Ç–æ–≤', 0)}")
                print(f"   üöó –í—Å–µ–≥–æ –¥–∏–ª–µ—Ä–æ–≤: {meta.get('–≤—Å–µ–≥–æ_–¥–∏–ª–µ—Ä–æ–≤', 0)}")
                print(f"   üè¢ –ü—Ä–µ–¥—Å—Ç–∞–≤–∏—Ç–µ–ª—å—Å—Ç–≤: {meta.get('–Ω–∞–π–¥–µ–Ω–æ_–ø—Ä–µ–¥—Å—Ç–∞–≤–∏—Ç–µ–ª—å—Å—Ç–≤', 0)}")  # –ù–û–í–û–ï
                print(f"   üìû –°—Ç—Ä–∞–Ω–∏—Ü —Å –∫–æ–Ω—Ç–∞–∫—Ç–∞–º–∏: {meta.get('–Ω–∞–π–¥–µ–Ω–æ_—Å—Ç—Ä–∞–Ω–∏—Ü_—Å_–∫–æ–Ω—Ç–∞–∫—Ç–∞–º–∏', 0)}")

                # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
                filename = f"enhanced_analysis_{urlparse(site).netloc}.json"
                with open(filename, 'w', encoding='utf-8') as f:
                    json.dump(result, f, ensure_ascii=False, indent=2)
                print(f"\nüíæ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {filename}")

        except Exception as e:
            print(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}")


if __name__ == "__main__":
    main()