import requests
from bs4 import BeautifulSoup
import time
import json
import re
from typing import List, Dict, Optional
import urllib3
from urllib.parse import urljoin
import logging

# –û—Ç–∫–ª—é—á–∞–µ–º –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è SSL
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


class FinalMoscowCompaniesParser:
    def __init__(self):
        self.session = requests.Session()
        self.session.verify = False
        self.base_url = "https://www.list-org.com"
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        })

    def get_search_page(self, page: int = 1) -> Optional[str]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã —Å –ø–æ–∏—Å–∫–æ–≤—ã–º–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏"""
        try:
            url = f"{self.base_url}/search"
            params = {'type': 'all', 'val': '–º–æ—Å–∫–≤–∞', 'page': page}
            response = self.session.get(url, params=params, timeout=30)
            response.raise_for_status()
            return response.text
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Å—Ç—Ä–∞–Ω–∏—Ü—ã {page}: {e}")
            return None

    def parse_companies(self, html: str) -> List[Dict]:
        """–ü–∞—Ä—Å–∏–Ω–≥ –∫–æ–º–ø–∞–Ω–∏–π –∏–∑ HTML"""
        soup = BeautifulSoup(html, 'html.parser')
        companies = []
        
        # –ù–∞—Ö–æ–¥–∏–º –≤—Å–µ label —ç–ª–µ–º–µ–Ω—Ç—ã —Å –∫–æ–º–ø–∞–Ω–∏—è–º–∏
        labels_with_companies = []
        for label in soup.find_all('label'):
            if label.find('a', href=re.compile(r'^/company/\d+')):
                labels_with_companies.append(label)
        
        logger.info(f"–ù–∞–π–¥–µ–Ω–æ —ç–ª–µ–º–µ–Ω—Ç–æ–≤ —Å –∫–æ–º–ø–∞–Ω–∏—è–º–∏: {len(labels_with_companies)}")
        
        for label in labels_with_companies:
            company_data = self.parse_company_label(label)
            if company_data and company_data.get('inn'):
                companies.append(company_data)
        
        return companies

    def parse_company_label(self, label) -> Optional[Dict]:
        """–ü–∞—Ä—Å–∏–Ω–≥ –¥–∞–Ω–Ω—ã—Ö –∫–æ–º–ø–∞–Ω–∏–∏ –∏–∑ label —ç–ª–µ–º–µ–Ω—Ç–∞"""
        try:
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –Ω–∞–∑–≤–∞–Ω–∏–µ –∏ —Å—Å—ã–ª–∫—É
            company_link = label.find('a', href=re.compile(r'^/company/\d+'))
            if not company_link:
                return None
                
            company_name = company_link.get_text(strip=True)
            company_url = urljoin(self.base_url, company_link.get('href', ''))
            
            # –ü–æ–ª—É—á–∞–µ–º –≤–µ—Å—å —Ç–µ–∫—Å—Ç label
            full_text = label.get_text()
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –ò–ù–ù (—Ñ–æ—Ä–º–∞—Ç: –∏–Ω–Ω/–∫–ø–ø: 7707294277/770701001)
            inn_match = re.search(r'–∏–Ω–Ω/–∫–ø–ø:\s*(\d{10})/\d{9}', full_text, re.IGNORECASE)
            inn = inn_match.group(1) if inn_match else None
            
            if not inn:
                return None
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –û–ì–†–ù (–º–æ–∂–µ—Ç –±—ã—Ç—å –≤ –¥—Ä—É–≥–æ–º —Ñ–æ—Ä–º–∞—Ç–µ)
            ogrn_match = re.search(r'–û–ì–†–ù\s*(\d{13})', full_text, re.IGNORECASE)
            ogrn = ogrn_match.group(1) if ogrn_match else None
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –∞–¥—Ä–µ—Å (—Ñ–æ—Ä–º–∞—Ç: —é—Ä.–∞–¥—Ä–µ—Å: 103159, –ì.–ú–æ—Å–∫–≤–∞, –£–õ. –û–•–û–¢–ù–´–ô –†–Ø–î, –î.2)
            address_match = re.search(r'—é—Ä\.–∞–¥—Ä–µ—Å:\s*([^\n]+)', full_text, re.IGNORECASE)
            address = address_match.group(1).strip() if address_match else None
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Å—Ç–∞—Ç—É—Å
            status = '–Ω–µ –¥–µ–π—Å—Ç–≤—É—é—â–µ–µ' if '–Ω–µ –¥–µ–π—Å—Ç–≤—É—é—â–µ–µ' in full_text else '–¥–µ–π—Å—Ç–≤—É—é—â–µ–µ'
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –ø–æ–ª–Ω–æ–µ –Ω–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ (–∏–¥–µ—Ç –ø–æ—Å–ª–µ —Å—Ç–∞—Ç—É—Å–∞)
            full_name_match = re.search(r'–Ω–µ –¥–µ–π—Å—Ç–≤—É—é—â–µ–µ([^—Ä—É–∫–æ–≤–æ–¥–∏—Ç–µ–ª—å]+)', full_text)
            if not full_name_match:
                full_name_match = re.search(r'–¥–µ–π—Å—Ç–≤—É—é—â–µ–µ([^—Ä—É–∫–æ–≤–æ–¥–∏—Ç–µ–ª—å]+)', full_text)
            
            full_name = full_name_match.group(1).strip() if full_name_match else None
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ä—É–∫–æ–≤–æ–¥–∏—Ç–µ–ª—è
            director_match = re.search(r'—Ä—É–∫–æ–≤–æ–¥–∏—Ç–µ–ª—å:\s*([^\n]+?)(?=–∏–Ω–Ω|—é—Ä\.–∞–¥—Ä–µ—Å|$)', full_text, re.IGNORECASE)
            director = director_match.group(1).strip() if director_match else None
            
            return {
                'name': company_name,
                'full_name': full_name,
                'inn': inn,
                'ogrn': ogrn,
                'address': address,
                'director': director,
                'status': status,
                'url': company_url,
                'parsed_at': time.strftime('%Y-%m-%d %H:%M:%S')
            }
            
        except Exception as e:
            logger.warning(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –∫–æ–º–ø–∞–Ω–∏–∏ {company_name if 'company_name' in locals() else 'N/A'}: {e}")
            return None

    def has_next_page(self, html: str, current_page: int) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è —Å–ª–µ–¥—É—é—â–µ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã"""
        soup = BeautifulSoup(html, 'html.parser')
        
        # –ò—â–µ–º —Å—Å—ã–ª–∫–∏ —Å –Ω–æ–º–µ—Ä–æ–º —Å–ª–µ–¥—É—é—â–µ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã
        next_links = soup.find_all('a', href=re.compile(r'page=\d+'))
        for link in next_links:
            href = link.get('href', '')
            page_match = re.search(r'page=(\d+)', href)
            if page_match:
                page_num = int(page_match.group(1))
                if page_num == current_page + 1:
                    return True
                    
        return False

    def collect_companies(self, max_pages: int = 5) -> List[Dict]:
        """–°–±–æ—Ä –∫–æ–º–ø–∞–Ω–∏–π —Å –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —Å—Ç—Ä–∞–Ω–∏—Ü"""
        all_companies = []
        
        for page in range(1, max_pages + 1):
            logger.info(f"üìÑ –û–±—Ä–∞–±–∞—Ç—ã–≤–∞—é —Å—Ç—Ä–∞–Ω–∏—Ü—É {page}...")
            
            html = self.get_search_page(page)
            if not html:
                logger.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å —Å—Ç—Ä–∞–Ω–∏—Ü—É {page}")
                break
                
            companies = self.parse_companies(html)
            all_companies.extend(companies)
            
            logger.info(f"‚úÖ –ù–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ {page} –Ω–∞–π–¥–µ–Ω–æ {len(companies)} –∫–æ–º–ø–∞–Ω–∏–π")
            logger.info(f"üìä –í—Å–µ–≥–æ —Å–æ–±—Ä–∞–Ω–æ: {len(all_companies)} –∫–æ–º–ø–∞–Ω–∏–π")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –µ—Å—Ç—å –ª–∏ —Å–ª–µ–¥—É—é—â–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞
            if page < max_pages and self.has_next_page(html, page):
                time.sleep(2)  # –ü–∞—É–∑–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
            else:
                logger.info("üèÅ –î–æ—Å—Ç–∏–≥–Ω—É—Ç–∞ –ø–æ—Å–ª–µ–¥–Ω—è—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞")
                break
                
        return all_companies

    def save_results(self, companies: List[Dict]):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ —Ñ–∞–π–ª—ã"""
        if not companies:
            logger.warning("‚ùå –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è")
            return
            
        timestamp = time.strftime("%Y%m%d_%H%M%S")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø–æ–ª–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –≤ JSON
        json_filename = f"moscow_companies_{timestamp}.json"
        with open(json_filename, 'w', encoding='utf-8') as f:
            json.dump(companies, f, ensure_ascii=False, indent=2)
        logger.info(f"üíæ –ü–æ–ª–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {json_filename}")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–æ–ª—å–∫–æ –ò–ù–ù –≤ —Ç–µ–∫—Å—Ç–æ–≤—ã–π —Ñ–∞–π–ª
        inns = [company['inn'] for company in companies if company.get('inn')]
        inns_filename = f"moscow_inns_{timestamp}.txt"
        with open(inns_filename, 'w', encoding='utf-8') as f:
            for inn in inns:
                f.write(f"{inn}\n")
        logger.info(f"üìù –°–ø–∏—Å–æ–∫ –ò–ù–ù —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ {inns_filename} ({len(inns)} –∑–∞–ø–∏—Å–µ–π)")

    def print_statistics(self, companies: List[Dict]):
        """–í—ã–≤–æ–¥ –ø–æ–¥—Ä–æ–±–Ω–æ–π —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏"""
        print("\n" + "="*70)
        print("–°–¢–ê–¢–ò–°–¢–ò–ö–ê –°–ë–û–†–ê –ö–û–ú–ü–ê–ù–ò–ô –ú–û–°–ö–í–´")
        print("="*70)
        
        total = len(companies)
        if total == 0:
            print("‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω–æ –Ω–∏ –æ–¥–Ω–æ–π –∫–æ–º–ø–∞–Ω–∏–∏")
            return
            
        with_inn = len([c for c in companies if c.get('inn')])
        with_ogrn = len([c for c in companies if c.get('ogrn')])
        with_address = len([c for c in companies if c.get('address')])
        with_director = len([c for c in companies if c.get('director')])
        active_companies = len([c for c in companies if c.get('status') == '–¥–µ–π—Å—Ç–≤—É—é—â–µ–µ'])
        inactive_companies = len([c for c in companies if c.get('status') == '–Ω–µ –¥–µ–π—Å—Ç–≤—É—é—â–µ–µ'])
        
        print(f"üìà –í—Å–µ–≥–æ –∫–æ–º–ø–∞–Ω–∏–π: {total}")
        print(f"üî¢ –° –ò–ù–ù: {with_inn} ({with_inn/total*100:.1f}%)")
        print(f"üìã –° –û–ì–†–ù: {with_ogrn} ({with_ogrn/total*100:.1f}%)")
        print(f"üè† –° –∞–¥—Ä–µ—Å–æ–º: {with_address} ({with_address/total*100:.1f}%)")
        print(f"üë§ –° —Ä—É–∫–æ–≤–æ–¥–∏—Ç–µ–ª–µ–º: {with_director} ({with_director/total*100:.1f}%)")
        print(f"‚úÖ –î–µ–π—Å—Ç–≤—É—é—â–∏–µ: {active_companies} ({active_companies/total*100:.1f}%)")
        print(f"‚ùå –ù–µ –¥–µ–π—Å—Ç–≤—É—é—â–∏–µ: {inactive_companies} ({inactive_companies/total*100:.1f}%)")
        
        print(f"\nüìã –ü–µ—Ä–≤—ã–µ 5 –∫–æ–º–ø–∞–Ω–∏–π:")
        print("-" * 70)
        for i, company in enumerate(companies[:5], 1):
            print(f"{i}. {company['name']}")
            print(f"   üÜî –ò–ù–ù: {company.get('inn', '–Ω–µ —É–∫–∞–∑–∞–Ω')}")
            if company.get('full_name'):
                print(f"   üìõ –ü–æ–ª–Ω–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ: {company['full_name']}")
            if company.get('address'):
                print(f"   üìç –ê–¥—Ä–µ—Å: {company['address']}")
            if company.get('director'):
                print(f"   üë§ –†—É–∫–æ–≤–æ–¥–∏—Ç–µ–ª—å: {company['director']}")
            if company.get('status'):
                print(f"   üìä –°—Ç–∞—Ç—É—Å: {company['status']}")
            if company.get('url'):
                print(f"   üîó –°—Å—ã–ª–∫–∞: {company['url']}")
            print()
            
        print("="*70)


def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    parser = FinalMoscowCompaniesParser()
    
    try:
        logger.info("üöÄ –ó–∞–ø—É—Å–∫ —Å–±–æ—Ä–∞ –∫–æ–º–ø–∞–Ω–∏–π –ú–æ—Å–∫–≤—ã...")
        
        # –°–æ–±–∏—Ä–∞–µ–º –∫–æ–º–ø–∞–Ω–∏–∏
        companies = parser.collect_companies(max_pages=5)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        parser.save_results(companies)
        
        # –í—ã–≤–æ–¥–∏–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        parser.print_statistics(companies)
        
        return companies
        
    except KeyboardInterrupt:
        logger.info("‚èπ –°–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –ø—Ä–µ—Ä–≤–∞–Ω –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
        return []
    except Exception as e:
        logger.error(f"üí• –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}")
        return []


if __name__ == "__main__":
    main()